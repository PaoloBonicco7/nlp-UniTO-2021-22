{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Es.1 - Conceptual Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo esercizio andiamo ad esplorare varie tecniche per calcolare la similarità semantica tra \n",
    "due parole. Per farlo utilizzeremo il dataset WordSim353.csv, che contiene una serie di coppie di\n",
    " parole e il loro punteggio di similarità. Il punteggio è un valore reale compreso tra 0 e 10.\n",
    "\n",
    "Le misure di similarità che utilizzeremo sono:\n",
    "\n",
    "- Wu and Palmer\n",
    "- Shortest Path\n",
    "- Leakcock & Chodorow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "import random\n",
    "from iteration_utilities import deepflatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_triple(path):\n",
    "    '''\n",
    "    Read a file and return the first 2 word in each row, as a tuple.\n",
    "    '''\n",
    "    tuple_list = []\n",
    "    with open (path, 'r') as f:\n",
    "        for row in f:\n",
    "            # Remove \\n\n",
    "            row = row.strip()\n",
    "            # Organize in a triple the values\n",
    "            tuple_list.append(tuple(row.split(\",\")[:3]))\n",
    "            \n",
    "        # Remove first value of the tuple\n",
    "        tuple_list.pop(0)\n",
    "    f.close()\n",
    "    return tuple_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = extract_triple('../data/WordSim353.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordNet Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synsets(term):\n",
    "    '''\n",
    "    Retrurn the synsets of a term.\n",
    "    '''\n",
    "    if(len(wn.synsets(term)) > 0):\n",
    "        return wn.synsets(term)\n",
    "    return None\n",
    "\n",
    "def wn_max_depth(syn): \n",
    "    '''\n",
    "    Return the depth of a synset, that is the distance between the\n",
    "    synset and the root.\n",
    "    '''\n",
    "    if(syn is None):\n",
    "            return 0\n",
    "    # Gestisco il caso in cui gli passo una lista con un solo elemento, il sysnset\n",
    "    if(type(syn) is list): \n",
    "        return syn[0].max_depth()\n",
    "    return syn.max_depth()\n",
    "    \n",
    "\n",
    "def lch(syn1, syn2): #? WordNet function\n",
    "    '''\n",
    "    Return the lowest common hypernyms of two synsets.\n",
    "    '''\n",
    "    if(syn1 is None or syn2 is None):\n",
    "        return None\n",
    "    return syn1.lowest_common_hypernyms(syn2)\n",
    "\n",
    "def lowest_common_subsumer(synset1, synset2): #? My function taht simulate the WordNet function\n",
    "    '''\n",
    "    Args:\n",
    "        synset1: first synset to take LCS from\n",
    "        synset2: second synset to take LCS from\n",
    "    Returns:\n",
    "        the first common LCS\n",
    "    '''\n",
    "    if synset2 == synset1:\n",
    "        return synset2\n",
    "\n",
    "    commonsArr = []\n",
    "    for hyper1 in synset1.hypernym_paths():\n",
    "        for hyper2 in synset2.hypernym_paths():\n",
    "            zipped = list(zip(hyper1, hyper2))  # merges 2 list in one list of tuples\n",
    "            common = None\n",
    "            for i in range(len(zipped)):\n",
    "                if(zipped[i][0] != zipped[i][1]):\n",
    "                    break\n",
    "                common = (zipped[i][0], i)\n",
    "\n",
    "            if common is not None and common not in commonsArr:\n",
    "                commonsArr.append(common)\n",
    "    \n",
    "    if len(commonsArr) <= 0:\n",
    "        return None\n",
    "\n",
    "    commonsArr.sort(key=lambda x: x[1], reverse=True)\n",
    "    return commonsArr[0][0]\n",
    "\n",
    "def wu_pal_sim(syn1, syn2): #? WordNet function\n",
    "    '''\n",
    "    Return the Wu-Palmer similarity of two synsets.\n",
    "    '''\n",
    "    if(syn1 is None or syn2 is None):\n",
    "        return 0\n",
    "    return syn1.wup_similarity(syn2)\n",
    "\n",
    "def my_wu_pal_sim(syn1, syn2): #? My function that simulate the WordNet function\n",
    "    '''\n",
    "    IMplementation of Wu and Palm similarity metrics\n",
    "    '''\n",
    "    lcs = (lowest_common_subsumer(syn1, syn2))\n",
    "    if lcs is None:\n",
    "        return 0\n",
    "\n",
    "    depth_lcs = depth_path(lcs, lcs)\n",
    "    depth_s1 = depth_path(syn1, lcs)\n",
    "    depth_s2 = depth_path(syn2, lcs)\n",
    "\n",
    "    result = (2 * depth_lcs) / (depth_s1 + depth_s2)\n",
    "    return result * 10\n",
    "\n",
    "    \n",
    "def max_similarity(syns1, syns2):\n",
    "    '''\n",
    "    The method comprare all the synsets of the 2 term and return the\n",
    "    synsets with the highest similarity with the relative score.\n",
    "    \n",
    "    Args:\n",
    "        syns1: list of synsets of the first term\n",
    "        syns2: list of synsets of the second term\n",
    "    Returns:\n",
    "        the higher value for similarity and the relative synsets\n",
    "    '''\n",
    "    sim_max = (\"\", \"\", 0)\n",
    "    \n",
    "    for syn1 in syns1:\n",
    "        for syn2 in syns2:\n",
    "            sim = my_wu_pal_sim(syn1, syn2)\n",
    "            if sim >= sim_max[2]:\n",
    "                sim_max = (syn1, syn2, sim)      \n",
    "                        \n",
    "    return sim_max\n",
    "\n",
    "def depth_path(synset, lcs):\n",
    "    \"\"\"It measures the distance (depth) between the given Synset and the WordNet's root.\n",
    "    Args:\n",
    "        synset: synset to reach from the root\n",
    "        lcs: Lowest Common Subsumer - the first common sense or most specific ancestor node\n",
    "    Returns:\n",
    "        the minimum path which contains LCS\n",
    "    \"\"\"\n",
    "    paths = synset.hypernym_paths()\n",
    "    paths = list(filter(lambda x: lcs in x, paths))  # all path containing LCS\n",
    "    return min(len(path) for path in paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wu and Palmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Empty Synsets: Maradona football\n",
      "SIMILARiTY = 0. Real value by WUP of  Synset('investor.n.01') ; Synset('earn.v.02') : 0.18181818181818182\n"
     ]
    }
   ],
   "source": [
    "for val in values:\n",
    "    syns1 = get_synsets(val[0])\n",
    "    syns2 = get_synsets(val[1])\n",
    "    \n",
    "    if(syns1 is not None and syns2 is not None):\n",
    "        sim = max_similarity(syns1, syns2)\n",
    "        if(sim[2] == 0):\n",
    "            print(\"SIMILARiTY = 0. Real value by WUP of \", sim[0], \";\", sim[1], \":\", wu_pal_sim(sim[0], sim[1]))\n",
    "    else:\n",
    "        print(\"-- Empty Synsets:\", val[0], val[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortest Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the maximum distance in WordNet tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_path():\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        The max depth of WordNet tree (20)\n",
    "    \"\"\"\n",
    "    max_path = 0\n",
    "    for synset in wn.all_synsets():\n",
    "        if synset.max_depth() > max_path:\n",
    "            max_path = synset.max_depth()\n",
    "    return max_path\n",
    "\n",
    "def max_path_2(): #19\n",
    "    return max(max(len(hyp_path) for hyp_path in ss.hypernym_paths()) for ss in wn.all_synsets())\n",
    "\n",
    "# max_depth = max_path() #! take too much time to compute\n",
    "max_depth = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the distence method between two words in WordNet, for doing that we define a\n",
    "function that evaluate the lowest_common_subsumer, but without using the built-in function\n",
    "*lowest_common_hypernyms* of wordnet.\n",
    "\n",
    "We also use *hypenym_paths* fucntion that return all the tree paths between the root and\n",
    "the given synset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(synset1, synset2):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        synset1: first synset to calculate distance\n",
    "        synset2: second synset to calculate\n",
    "    Returns:\n",
    "        distance between the two synset\n",
    "    \"\"\"\n",
    "    lcs = lowest_common_subsumer(synset1, synset2)\n",
    "    if lcs is None:\n",
    "        return None\n",
    "\n",
    "    hypernym1 = synset1.hypernym_paths()\n",
    "    hypernym2 = synset2.hypernym_paths()\n",
    "\n",
    "    # paths from LCS to root\n",
    "    hypernym_lcs = lcs.hypernym_paths()\n",
    "\n",
    "    # create a set of unique items flattening the nested list\n",
    "    set_lcs = set(deepflatten(hypernym_lcs))\n",
    "\n",
    "    # remove root\n",
    "    set_lcs.remove(lcs)\n",
    "\n",
    "    # path from synset to LCS\n",
    "    hypernym1 = list(map(lambda x: [y for y in x if y not in set_lcs], hypernym1))\n",
    "    hypernym2 = list(map(lambda x: [y for y in x if y not in set_lcs], hypernym2))\n",
    "\n",
    "    # path containing LCS\n",
    "    hypernym1 = list(filter(lambda x: lcs in x, hypernym1))\n",
    "    hypernym2 = list(filter(lambda x: lcs in x, hypernym2))\n",
    "\n",
    "    return min(list(map(lambda x: len(x), hypernym1))) + min(list(map(lambda x: len(x), hypernym2))) - 2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venvradicioni': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "585c611d1963e3d050f9aad8a09a6992370122bcceddf6a7b91b57ac2b60585c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
