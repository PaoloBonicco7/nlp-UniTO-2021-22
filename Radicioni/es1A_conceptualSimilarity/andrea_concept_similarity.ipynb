{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.nltk.org/howto/wordnet.html WordNet examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synsets(word):\n",
    "    return wn.synsets(word)\n",
    "\n",
    "def get_all_hypernyms(synset):\n",
    "    # get all hypernyms of a synset until the root of wordnet\n",
    "    # estendere per far prendere più iperonimi, non solo il primo ogni volta\n",
    "    ret_list = []\n",
    "    hypernyms = synset.hypernyms()\n",
    "    while hypernyms:\n",
    "        for hyper in hypernyms:\n",
    "            ret_list.append(hyper)\n",
    "        hypernyms = hypernyms[0].hypernyms()\n",
    "    return ret_list\n",
    "\n",
    "def lowest_common_subsumer(syn1, syn2):\n",
    "    return syn1.lowest_common_hypernyms(syn2)[0] if syn1.lowest_common_hypernyms(syn2) else None\n",
    "\n",
    "def depth(syn):\n",
    "    return syn.min_depth() if syn else 0\n",
    "\n",
    "def lowest_common_subsumer_2(syn1, syn2):\n",
    "    # risale la gerarchia degli iperonimi, scegliendo sempre e solo il primo synset percjè non ho un PC non della NASA\n",
    "    syn1_hypernyms = get_all_hypernyms(syn1)\n",
    "    syn2_hypernyms = get_all_hypernyms(syn2)\n",
    "\n",
    "    for h1 in syn1_hypernyms:\n",
    "        if h1 in syn2_hypernyms:\n",
    "            return h1\n",
    "\n",
    "def max_path(): # restituisce sempre 19, per velocizzare le esecuzioni salvo il valore in una costante\n",
    "    max_path = 0\n",
    "    for synset in wn.all_synsets():\n",
    "        if synset.max_depth() > max_path:\n",
    "            max_path = synset.max_depth()\n",
    "    return max_path\n",
    "\n",
    "def length(syn1, syn2): # NB, non esistono i cammini tra nomi e verbi in WordNet, pepr cui vanno rimossi i verbi credo\n",
    "    return syn1.shortest_path_distance(syn2) if syn1.shortest_path_distance(syn2) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PATH = 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wu & Palmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wu_palmer(syn1, syn2):\n",
    "    dep = 0\n",
    "\n",
    "    lcs = lowest_common_subsumer_2(syn1, syn2)\n",
    "    dep = (depth(syn1) + depth(syn2))\n",
    "    \n",
    "    if dep == 0:\n",
    "        dep = 0.001\n",
    "        \n",
    "    return 2 * depth(lcs) / dep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortest Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest_path(syn1, syn2):\n",
    "    return 2 * MAX_PATH - length(syn1, syn2) if length(syn1, syn2) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leakcock & Chodorow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leakcock_chodorow(syn1, syn2):\n",
    "    return -np.log(length(syn1, syn2) / 2 * MAX_PATH) if length(syn1, syn2) else -1000 #-1000 per indicare un valore di somiglianza basso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read lines from WordSim353.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "with open(r'C:\\Users\\andre\\Desktop\\Università\\Magistrale\\TLN\\TLN-LAB\\nlp-UniTO-2021-22\\Radicioni\\data\\WordSim353.csv', 'r') as f:\n",
    "    word_sim = f.readlines()[1:]\n",
    "    for tuple in word_sim:\n",
    "        dataset.append(tuple.split(','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get synset from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns_1 = []\n",
    "syns_2 = []\n",
    "\n",
    "for tuple in dataset:\n",
    "    syns_1.append(get_synsets(tuple[0]))\n",
    "    syns_2.append(get_synsets(tuple[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute similarity using the three methods described above over all the combinations of synsets of every word in the input file.\n",
    "For each couple, take the maximum value of each similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp = []\n",
    "sp = []\n",
    "lc = []\n",
    "\n",
    "max_wu = 0\n",
    "max_sp =  0\n",
    "max_lc = -1000\n",
    "\n",
    "for i in range(len(syns_1)):\n",
    "    for j in range(len(syns_1[i])):\n",
    "        for k in range(len(syns_2[i])):\n",
    "            \n",
    "            #print(f\"syn1: {syns_1[i][j]}, syn2: {syns_2[i][k]} --> WU_PALMER: {wu_palmer(syns_1[i][j], syns_2[i][k])} - SHORTEST_PATH: {shortest_path(syns_1[i][j], syns_2[i][k])} - LEAKCOCK_CHODOROW: {leakcock_chodorow(syns_1[i][j], syns_2[i][k])}\")\n",
    "            if wu_palmer(syns_1[i][j], syns_2[i][k]) > float(max_wu):\n",
    "                max_wu = wu_palmer(syns_1[i][j], syns_2[i][k])\n",
    "            if shortest_path(syns_1[i][j], syns_2[i][k]) > float(max_sp):\n",
    "                max_sp = shortest_path(syns_1[i][j], syns_2[i][k])\n",
    "            if leakcock_chodorow(syns_1[i][j], syns_2[i][k]) > float(max_lc):\n",
    "                max_lc = leakcock_chodorow(syns_1[i][j], syns_2[i][k])\n",
    "\n",
    "    wp.append(max_wu) \n",
    "    max_wu = 0\n",
    "    sp.append(max_sp)\n",
    "    max_sp = 0\n",
    "    lc.append(max_lc)\n",
    "    max_lc = -1000\n",
    "\n",
    "print(wp)\n",
    "print(sp)\n",
    "print(lc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluation methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6847c98a8f86b01c6a19c518cd2f366693b80566b266804d5ca763cbb223f52b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
