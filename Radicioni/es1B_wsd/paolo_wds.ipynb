{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Es2 - Word Sense Disambiguation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo esercizio andremo ad estrarre 50 frasi causuali dal corpus `SemCor` e proveremo a disambiguare un sostantivo per ogni frase, anche\n",
    "quest'ultimo estratto casualmente dalla frase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Estrazione casuale delle frasi dal corpus `SemCor`\n",
    "   \n",
    "2. Pulizia delle frasi:\n",
    "   \n",
    "   - Rimozione stopwords, punteggiatura e lemming\n",
    "  \n",
    "3. Estrazione di un sostantivo casuale dalla frase, che sia ambiguo, cioè che abbia associato \n",
    "   almeno 2 synset in wordnet\n",
    "\n",
    "4. Estrazione dei synset del sostantivo da wordnet e del synset corretto associato in `SemCor`\n",
    "   \n",
    "5. Costruzione della `Bag of Words` per la frase:\n",
    "   \n",
    "   - Creiamo un insieme delle parole correlate ai termini presenti nella frasi, per far ciò andiamo ad\n",
    "      estrarre le definizioni e gli esempi dei synset da wordnet, dati da `SemCor`, \n",
    "      associati ai termini della frase\n",
    "      \n",
    "6. Andiamo a confrontare la `Bag of Words` della frase con i termini presenti nelle definizioni e negli esempi\n",
    "   dei vari synset associati al sostantivo estratto e andiamo a selezionare il synset con *overlap* maggiore,\n",
    "   che dovrebbe corrispondere al synset corretto\n",
    "   \n",
    "7. Andiamo a calcolare le performance del nostro algoritmo andando a confrontare i sysnet estratti con\n",
    "   quelli corretti dati da `SemCor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparazione dei dati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and dataset downlaod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import semcor\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import random\n",
    "from pprint import pprint\n",
    "from nltk.tree import *\n",
    "from iteration_utilities import deepflatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resurces to install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('semcor') # download the semcor corpus\n",
    "# nltk.download('wordnet') # download wordnet\n",
    "# nltk.download('omw-1.4') # download the open multilingual wordnet\n",
    "# nltk.download('stopwords') # download the stopwords\n",
    "# nltk.download('punkt') # download the punkt tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrazione frasi da corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estraggo le frasi con  `semcor.sents()` e le frasi annotate con PoS tag e i Lemma con `semcor.tagged_sents()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "semc_sents = semcor.sents() # Extract the sentences from the semcor corpus\n",
    "\n",
    "semc_sents_full = semcor.tagged_sents(tag=\"both\") # Extract the sentences from the semcor corpus with annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metodi utili per gestione Corpus `SemCor` e struttura `Tree` di `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma(word):\n",
    "    '''\n",
    "    Args:\n",
    "        word: term as as nltk.Tree\n",
    "    Returns:\n",
    "        lemma of the word as a string.\n",
    "        If there isn't a lemma, return the PoS tag or None.\n",
    "    '''\n",
    "    return word.label()\n",
    "\n",
    "def get_word(word):\n",
    "    '''\n",
    "    Args:\n",
    "        word: term as as nltk.tree.tree.Tree\n",
    "    Returns:\n",
    "        the term as as a list of strings.\n",
    "        Return a list because a term may consist of several words\n",
    "    '''\n",
    "    if(isinstance(word, nltk.Tree)):\n",
    "        return word.leaves()\n",
    "    return None\n",
    "\n",
    "def get_pos(word):\n",
    "    '''\n",
    "    Args:\n",
    "        word: term as as nltk.Tree\n",
    "    Returns:\n",
    "        The PoS tag of a word or None if there is no PoS for the term \n",
    "        (ex. '!' has tag None)\n",
    "    '''\n",
    "    return word.pos()[0][1]\n",
    "\n",
    "def get_synset(lemma):\n",
    "    '''\n",
    "    Args:\n",
    "        lemma: Lemma of a word\n",
    "    Returns:\n",
    "        The synset associated to the lemma\n",
    "    '''\n",
    "    if(isinstance(lemma, nltk.corpus.reader.wordnet.Lemma)):\n",
    "        return lemma.synset()\n",
    "    return None\n",
    "\n",
    "def get_sents(semcor):\n",
    "    '''\n",
    "    Args:\n",
    "        semcor: Semcor corpus\n",
    "    Return:\n",
    "        a list of list of words. Each list of words is a sentence.\n",
    "    '''\n",
    "    return semcor.sents()\n",
    "\n",
    "def get_term(lemma):\n",
    "    '''\n",
    "    Args:\n",
    "        lemma: Lemma of a word\n",
    "    Returns:\n",
    "        The term associated to the lemma\n",
    "    '''\n",
    "    if(isinstance(lemma, nltk.corpus.reader.wordnet.Lemma)):\n",
    "        return lemma.name()\n",
    "    return None\n",
    "\n",
    "def get_synsets(term):\n",
    "    '''\n",
    "    Args:\n",
    "        term: a word as string\n",
    "    Returns:\n",
    "        The synsets associated to the word\n",
    "    '''\n",
    "    if(len(wn.synsets(term)) > 0):\n",
    "        return wn.synsets(term)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selezione delle frasi casuali"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estraiamo le frasi come stringhe e come oggetti `Tree` per poter ottenere anche il pos e il \n",
    "lemma associato ad un termine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sent(sent):\n",
    "    '''\n",
    "    Args:\n",
    "        sent: a sent annotated extracted from the semcor corpus (semcor.tagged_sents(tag=\"both\"))\n",
    "    Returns:\n",
    "        True if the sentence contain an ambiguous noun (NN), False otherwise\n",
    "        By ambiguous word we mean a word that has more than one synset in wordnet\n",
    "    '''\n",
    "    for el in sent:\n",
    "        if(get_pos(el) == 'NN'):\n",
    "            lemma = get_lemma(el)\n",
    "            if(isinstance(lemma, nltk.corpus.reader.wordnet.Lemma)):\n",
    "                term = get_term(lemma)\n",
    "                syns = wn.synsets(term)\n",
    "                if(len(syns) > 1):\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "def pick_sents(s, sfull, num):\n",
    "    '''\n",
    "    Args:\n",
    "        s: list of strings -> List of sentences\n",
    "        sfull: annotated sentences extracted from the semcor corpus (semcor.tagged_sents(tag=\"both\"))\n",
    "        num: int -> number of sentences to pick\n",
    "    Returns:\n",
    "        Return 'num' sentences and annotated sentences given in input wich contain an ambiguous noun\n",
    "    '''\n",
    "    rand_sents, rand_num, rand_full_sent = [], [], []\n",
    "    l = len(s) - 1\n",
    "    n = random.randint(0, l)\n",
    "    \n",
    "    while (len(rand_sents) < num):\n",
    "        while(n in rand_num):\n",
    "            n = random.randint(0, l)\n",
    "            \n",
    "        rand_num.append(n)\n",
    "        \n",
    "        if(check_sent(sfull[n])):\n",
    "            rand_sents.append(s[n])\n",
    "            rand_full_sent.append(sfull[n])\n",
    "    \n",
    "    return rand_sents, rand_full_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estraggo 50 frasi casuali dal corpus `SemCor`\n",
    "\n",
    "Ottengo una lista di oggetti *semcor* -> `nltk.corpus.reader.semcor.SemcorSentence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_list, sent_full_list = pick_sents(semc_sents, semc_sents_full, 50)\n",
    "\n",
    "# print(len(sent_list) == len(sent_full_list)) # 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementazione algoritmo di Lesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils.log import error\n",
    "\n",
    "def find_noun(sent):\n",
    "    '''\n",
    "    Args:\n",
    "        sent: annotated sentences extracted from the semcor corpus (semcor.tagged_sents(tag=\"both\"))\n",
    "    Returns:\n",
    "        A random ambiguous noun in the sentence\n",
    "    '''\n",
    "    noun_list = []\n",
    "    for el in sent:\n",
    "        if(get_pos(el) == 'NN'):\n",
    "            lemma = get_lemma(el)\n",
    "            if(isinstance(lemma, nltk.corpus.reader.wordnet.Lemma)):\n",
    "                term = get_term(lemma)\n",
    "                syns = wn.synsets(term)\n",
    "                if(len(syns) > 1):\n",
    "                    noun_list.append(el)\n",
    "    return noun_list[random.randint(0, len(noun_list) - 1)]\n",
    "\n",
    "\n",
    "def bag_of_word(sent):\n",
    "    '''\n",
    "    Transforms the given sentence according to the bag of words approach, apply lemmatization, \n",
    "    stop words and punctuation removal.\n",
    "    Args:\n",
    "        sent: sentences -> a list of words\n",
    "    Returns:\n",
    "        A list of words after the preprocessing (stop words and punctuation removal, lemmatization)\n",
    "    '''\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = {',', ';', '(', ')', '{', '}', ':', '?', '!', '.', '``', '*', '-'}\n",
    "    \n",
    "    # Returns the input word unchanged if it cannot be found in WordNet.\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    \n",
    "    # Return a tokenized copy of text, using NLTK’s recommended word tokenizer (Treebank + PunkSentence)\n",
    "    tokens = nltk.word_tokenize(sent)\n",
    "    tokens = list(filter(lambda x: x not in stop_words and x not in punctuation, tokens))\n",
    "    \n",
    "    return list(wnl.lemmatize(t.lower()) for t in tokens)\n",
    "\n",
    "\n",
    "def get_context(sent):\n",
    "    '''\n",
    "    Join the words of the sentence in a single string and call bag_of_word method\n",
    "    Args:\n",
    "        sent: sentences -> a list of words\n",
    "    Returns:\n",
    "        The context of a sentences, a list of words after the preprocessing (stop words and punctuation \n",
    "        removal, lemmatization)\n",
    "    '''\n",
    "    context = []\n",
    "    merged_sent = ' '.join(word for word in sent)\n",
    "    context.append(bag_of_word(merged_sent))\n",
    "    return context[0]\n",
    "\n",
    "\n",
    "def get_signature(syn):\n",
    "    '''\n",
    "    Args:\n",
    "        synset: a synset of a word\n",
    "    Returns:\n",
    "        A list of word (bag of words) formed by examples and gloss of the synset\n",
    "    '''\n",
    "    bof = bag_of_word(syn.definition())\n",
    "    for el in syn.examples():\n",
    "        bof.extend(bag_of_word(el))\n",
    "    return bof\n",
    "\n",
    "\n",
    "def get_overlap(s1, s2):\n",
    "    '''\n",
    "    Args:\n",
    "        s1: list of words\n",
    "        s2: list of words\n",
    "    Returns:\n",
    "        The number of words in s1 that are also in s2\n",
    "    '''\n",
    "    return len(set(s1).intersection(set(s2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lesk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `Contesto` - Insieme delle parole presenti nella frase\n",
    "2. `Signature` - Insieme della parole presenti nella definizione e negli esempi dei synset del termine da disambiguare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lesk(word, sentence):\n",
    "    max_overlap = -1\n",
    "    best_synset = None\n",
    "    context = get_context(sentence)\n",
    "    word_synsets = wn.synsets(word)\n",
    "    for syn in word_synsets:\n",
    "        signature = get_signature(syn)\n",
    "        overlap = get_overlap(signature, context)\n",
    "        if(overlap > max_overlap):\n",
    "            max_overlap = overlap\n",
    "            best_synset = syn\n",
    "    return best_synset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution e performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_2_iteration(n_sent):    \n",
    "    result = []\n",
    "    correct = 0\n",
    "    path_sim_mean = 0\n",
    "    num_sent = n_sent\n",
    "    \n",
    "    sent_list, sent_full_list = pick_sents(semc_sents, semc_sents_full, num_sent)\n",
    "\n",
    "    for i in range(num_sent):\n",
    "        target_word = find_noun(sent_full_list[i]) # find a random ambiguous noun in the sentence\n",
    "        lemma = get_lemma(target_word) # and get his lemma\n",
    "        target_word_str = get_term(lemma) # get the word as a string\n",
    "        target_synset = lemma.synset() # and the right synset associated to the word\n",
    "        sent = sent_list[i] # select the sentence\n",
    "        \n",
    "        lesk_syns = lesk(target_word_str, sent) # get the synset returned by the lesk algorithm\n",
    "        \n",
    "        path_sim_score = target_synset.path_similarity(lesk_syns) # compute the path similarity score\n",
    "            \n",
    "        partial_res = {}\n",
    "        path_sim_mean += path_sim_score\n",
    "        \n",
    "        if((type(lesk_syns) == type(target_synset)) and (lesk_syns == target_synset)):\n",
    "            correct += 1\n",
    "            partial_res = {\n",
    "                \"target_syn\": target_synset,\n",
    "                \"lesk_syn\": lesk_syns,\n",
    "                \"distance\": path_sim_score\n",
    "            }\n",
    "        else:\n",
    "            partial_res = {\n",
    "                \"target_syn\": target_synset,\n",
    "                \"lesk_syn\": lesk_syns,\n",
    "                \"distance\": path_sim_score\n",
    "            }   \n",
    "            \n",
    "        result.append(partial_res)\n",
    "    \n",
    "    path_sim_mean = path_sim_mean / num_sent  \n",
    "    \n",
    "    return result, (correct / num_sent), path_sim_mean\n",
    "\n",
    "\n",
    "# print(\"Correct: \", correct, \"out of\", num_sent, \"sentences\")\n",
    "# print(\"Accuracy: \", correct / num_sent)\n",
    "# print(\"Mean path similarity: \", path_sim_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy:  0.48080000000000017\n",
      "Average path similarity:  0.5384015556181453\n"
     ]
    }
   ],
   "source": [
    "iterations = 100\n",
    "average_accuracy = 0\n",
    "average_path_sim = 0\n",
    "num_sent = 50\n",
    "\n",
    "for k in range(iterations):\n",
    "    result, accuracy, path_sim_mean = es_2_iteration(num_sent)\n",
    "    average_accuracy += accuracy\n",
    "    average_path_sim += path_sim_mean\n",
    "    \n",
    "average_path_sim = average_path_sim / iterations\n",
    "average_accuracy = average_accuracy / iterations\n",
    "\n",
    "print(\"Average accuracy: \", average_accuracy)\n",
    "print(\"Average path similarity: \", average_path_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterazione di prova su una sola frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_list, sent_full_list = pick_sents(semc_sents, semc_sents_full, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word = find_noun(sent_full_list[0])\n",
    "lemma = get_lemma(target_word)\n",
    "target_word_str = get_term(lemma)\n",
    "target_synset = lemma.synset()\n",
    "\n",
    "sent_0 = sent_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Word: future\n",
      "\n",
      "Sentence: It is on them alone that the future of their race depends , for all their relatives ( mothers , husbands , brothers , and unmated sisters ) have perished with the arrival of the cold weather .\n",
      "\n",
      "Result: future.n.02 -- Correct: future.n.01\n"
     ]
    }
   ],
   "source": [
    "print(\"Target Word: \" + target_word_str)\n",
    "print(\"\\nSentence: \" + ' '.join(sent_0))\n",
    "\n",
    "lesk_syns = lesk(target_word_str, sent_0)\n",
    "\n",
    "print(\"\\nResult: \" + lesk_syns.name(), \"-- Correct: \" + target_synset.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venvradicioni': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe30bb084dadcc2974aabac99d8c6a813637b467837e0e94701709de5a097010"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
