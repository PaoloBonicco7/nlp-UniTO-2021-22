{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizio 3 - SemEval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consegna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parte 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Annotazione:** Annotare il file con i valori di similarità dati da noi e con i `BabelNetSynsetID` scelti da noi dal file `SemEval17_IT_senses2synsets.txt`.\n",
    "   \n",
    "2. **Agreement:** Dopo di che si calcola il `valore medio` di similarità, `Pearson` e `Spearman`\n",
    "   \n",
    "3. **Ricerca Automatica del Synset:** Andiamo a cercare il `BabelNetSynsetID` nel file dato dal prof `mini_NASARI.tsv`\n",
    "   andando a massimizzare la *cosine similarity*. I valori che massimizzeranno quest'ultima saranno i nostri concetti (Synset).\n",
    "\n",
    "   Per far ciò andiamo ad estrarre tutti i synsets relativi ad un termine, dal file `SemEval17_IT_senses2synsets.txt`\n",
    "   e andiamo a calcolare la *cosine similarity* per ogni synset (quelli dei due termini) presente nel file `mini_NASARI.tsv`\n",
    "   e salviamo i due sysnets che massimizzano la *cosine similarity*.\n",
    "\n",
    "4. **Valutazione Similarità:** Sempre con `Pearson` e `Spearman`, tra i nostri valori di similarità e quelli trovati\n",
    "   al passo precedente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Associare i synset ai termini che pensiamo che massimizzino la similarità (fatto a mano) --> Gia fatto al passo 1 \n",
    "\n",
    "2. Output costituito da 6 campi: \n",
    "   1. Term1\n",
    "   2. Term2\n",
    "   3. BS1 (dato da noi)\n",
    "   4. BS2 (dato da noi)\n",
    "   5. Sinonimi Term1 (da synset a lemma)\n",
    "   6. Sinonimi Term2 (da synset a lemma)\n",
    "\n",
    "   <br/>     \n",
    "3. **Agreement dell'anotazione Synset trovati:** Tramite il punteggio `kappa di Cohen` calcoliamo il livello \n",
    "   di agreement (sklearn) tra i synset trovati da noi\n",
    "\n",
    "4. **Valutazione annotazione:** Confronto i nostri concetti (synset) con quelli che massimizzano la `cos_sim`\n",
    "   (Trovati già al passo 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sviluppo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import csv\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1,2 - Annotazione e Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Organizzazione dei dati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo a selezionare il dati da utilizzare per l'esercizio 3 tramite la funzione data dal professore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = \"../data/es3_res/it.test.data.txt\"\n",
    "annotator = \"Bonicco\"\n",
    "data_file = \"SemEval_data_\" + annotator + \".tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonicco        :\tcoppie nell'intervallo 51-100\n"
     ]
    }
   ],
   "source": [
    "def get_range(surname):\n",
    "    nof_elements = 500\n",
    "    base_idx = (abs(int(hashlib.sha512(surname.encode('utf-8')).hexdigest(), 16)) % 10)\n",
    "    idx_intervallo = base_idx * 50+1\n",
    "    return idx_intervallo\n",
    " \n",
    "\n",
    "input_name = \"Bonicco\"\n",
    "\n",
    "values = []\n",
    "sx = get_range(input_name)\n",
    "values.append(sx)\n",
    "dx = sx+50-1\n",
    "intervallo = \"\" + str(sx) + \"-\" + str(dx)\n",
    "print('{:15}:\\tcoppie nell\\'intervallo {}'.format(input_name, intervallo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creazione file con le coppie di termini che ci interessano - **Non eseguire, file già creato**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['biotopo\\tbiologia', 'magma\\tvulcano', 'brainstorming\\ttelescopio', 'livello\\tpunteggio', 'centesimo\\taffare', 'partito politico\\tassociazione', 'tsunami\\tmare', 'struzzo\\tfrutteto', 'cannella\\tcaramella', 'scopa\\tpolvere', 'galassia\\tastronomo', 'succo\\tfrappè', 'tapparella\\ttenda', 'criminale\\tcolpevole', 'cancro al pancreas\\tchemioterapia', 'passato\\tantecedente', 'Nazioni Unite\\tParlamento Europeo', 'canzone\\tesecutore', 'pistola\\ttaccuino', 'acetilcolina\\tiride', 'alfabeto\\tpenna', 'coro\\tcantante', 'legno\\tcoperta', 'soldato\\tpace', 'inglese\\tamericano', 'lucertola\\tcoccodrillo', 'denaro\\tcontante', 'Polpo Paul\\tpolpo', 'calendario\\tvacanza', 'base\\tsostanza chimica', 'governo\\tlucertola', 'JPY\\triciclaggio di denaro', 'DeepMind\\tGoogle DeepMind', 'scacchi\\tscacco al re', 'sonetto\\tbellezza', 'lavoratore\\tufficio', 'valuta\\tscambio', 'poliestere\\tcotone', 'tribunale\\tgiustizia', 'coda\\tretro', 'incidente\\tlibro', 'groppo\\tvento', 'piano\\tarmonia', 'sultano\\tministero', 'deserto\\tduna', 'moltiplicazione\\tdivisione', 'virus\\tsangue', 'era glaciale\\tstatuto', 'spada\\tambiente', 'scuola elementare\\tpenna']\n"
     ]
    }
   ],
   "source": [
    "with open(path_data) as file:\n",
    "    lines = file.readlines()[50:100]\n",
    "    lines = [line.rstrip() for line in lines]\n",
    "    print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel seguente file excel abbiamo annotato i punteggi di similarità relativi alla coppia di termini, dopo di che abbiamo calcolato la media tra i valori dati\n",
    "dai due annotatori, oltre ai due indici di correlazione *Pearson* e *Spearman*.\n",
    "\n",
    "Inoltre, abbiamo anche annotato i synset relativi ai termini, andandoli a scegliere tra la lista dei synset data dal professore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://1drv.ms/x/s!AkdbdBkdl_1dhbcgYl1tXDNpjknAIw?e=UYsHF5\"> Link all'excel con i dati </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calcolo Spearman per Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_index_np(x, y): # [3,45 ; 5,66 ; ..]\n",
    "    '''\n",
    "    Implementation of the Pearson index using numpy\n",
    "    Args:\n",
    "         x: golden value\n",
    "         y: similarity list\n",
    "    Returns:\n",
    "        Pearson correlation index = [Covariance / (Standard deviation of x * Standard deviation of y)]\n",
    "    '''\n",
    "    mu_x = np.mean(x)\n",
    "    mu_y = np.mean(y)\n",
    "    \n",
    "    # and the standard deviation of both\n",
    "    std_dev_x = np.std(x)\n",
    "    std_dev_y = np.std(y)\n",
    "    \n",
    "    numeric = std_dev_x * std_dev_y\n",
    "\n",
    "    num = np.cov(x, y)[0][1] # Covariance    \n",
    "\n",
    "    return num / numeric\n",
    "\n",
    "def spearman_index(x, y):\n",
    "    '''\n",
    "    Implementation of the Spearman index.\n",
    "    Args:\n",
    "        x: golden value\n",
    "        y: similarity list\n",
    "    Returns:\n",
    "         Spearman correlation index\n",
    "    '''\n",
    "    rank__x = define_rank(x)\n",
    "    rank__y = define_rank(y)\n",
    "\n",
    "    return pearson_index_np(rank__x, rank__y)\n",
    "\n",
    "\n",
    "def define_rank(vector):\n",
    "    '''\n",
    "    Args:\n",
    "        vector: numeric vector\n",
    "    Returns:\n",
    "        ranks list, sorted as the input order\n",
    "    '''\n",
    "    x_couple = [(vector[i], i) for i in range(len(vector))]\n",
    "    x_couple_sorted = sorted(x_couple, key=lambda x: x[0])\n",
    "\n",
    "    return [y for (x, y) in x_couple_sorted]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### da togleire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "sim_paolo = [2,2.6,0,1.8,0.8,1.5,3,0,1,2.5,2.8,1,2.4,2.8,2.2,4,1.4,1.6,0,0.2,0.5,3,0,1.8,2,2.6,3,3,1,3,0,1.4,4,3.6,0.4,3,2.5,2,2.8,0.4,0,3,2.8,1.8,3.4,3.6,1.8,0,0,2.4]\n",
    "sim_davide = [1.8,2,0,1.8,1.2,2.8,1,0,0.8,1,1,3,1.5,2.8,1.8,3.5,0.8,0,0,0.5,1,3,0.8,1,3.2,3,4,2.5,1,2,0,1.2,4,2,0,2,2,3,2,3.8,0,2,1.5,1.2,2,3,1.5,0,0,1.2]\n",
    "\n",
    "print(len(sim_paolo) == len(sim_davide) == 50)\n",
    "\n",
    "# sim_mean = [1.9, 2.3, 0, 1.8, 1, 2.15, 2, 0, 0.9, 1.75, 1.9, 2, 1.95, 2.8, 2, 3.75, 1.1, 0.8, 0, 0.35, 0.75, 3, 0.4, 1.4, 2.6, 2.8, 3.5, 2.75, 1, 2.5, 0, 1.3, 4, 2.8, 0.2, 2.5, 2.25, 2.5, 2.4, 2.1, 0, 2.5, 2.15,1.5, 2.7, 3.3, 1.65, 0, 0, 1.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1991817135017272"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearman_index(sim_paolo, sim_davide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ricerca automatica del Synset - Massimizzazione cosine similarity da mini_NASARI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open(data_file, 'w').close() # Puliamo il file con i termini di riferimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['biotopo', 'biologia']\n",
      "['magma', 'vulcano']\n",
      "['brainstorming', 'telescopio']\n",
      "['livello', 'punteggio']\n",
      "['centesimo', 'affare']\n",
      "['partito politico', 'associazione']\n",
      "['tsunami', 'mare']\n",
      "['struzzo', 'frutteto']\n",
      "['cannella', 'caramella']\n",
      "['scopa', 'polvere']\n",
      "['galassia', 'astronomo']\n",
      "['succo', 'frappè']\n",
      "['tapparella', 'tenda']\n",
      "['criminale', 'colpevole']\n",
      "['cancro al pancreas', 'chemioterapia']\n",
      "['passato', 'antecedente']\n",
      "['Nazioni Unite', 'Parlamento Europeo']\n",
      "['canzone', 'esecutore']\n",
      "['pistola', 'taccuino']\n",
      "['acetilcolina', 'iride']\n",
      "['alfabeto', 'penna']\n",
      "['coro', 'cantante']\n",
      "['legno', 'coperta']\n",
      "['soldato', 'pace']\n",
      "['inglese', 'americano']\n",
      "['lucertola', 'coccodrillo']\n",
      "['denaro', 'contante']\n",
      "['Polpo Paul', 'polpo']\n",
      "['calendario', 'vacanza']\n",
      "['base', 'sostanza chimica']\n",
      "['governo', 'lucertola']\n",
      "['JPY', 'riciclaggio di denaro']\n",
      "['DeepMind', 'Google DeepMind']\n",
      "['scacchi', 'scacco al re']\n",
      "['sonetto', 'bellezza']\n",
      "['lavoratore', 'ufficio']\n",
      "['valuta', 'scambio']\n",
      "['poliestere', 'cotone']\n",
      "['tribunale', 'giustizia']\n",
      "['coda', 'retro']\n",
      "['incidente', 'libro']\n",
      "['groppo', 'vento']\n",
      "['piano', 'armonia']\n",
      "['sultano', 'ministero']\n",
      "['deserto', 'duna']\n",
      "['moltiplicazione', 'divisione']\n",
      "['virus', 'sangue']\n",
      "['era glaciale', 'statuto']\n",
      "['spada', 'ambiente']\n",
      "['scuola elementare', 'penna']\n"
     ]
    }
   ],
   "source": [
    "terms_sim_score = []\n",
    "\n",
    "with open(data_file, 'wt') as out_file:\n",
    "    tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "    tsv_writer.writerow([\"Term1\", \"Term2\", \"Similarity\"])\n",
    "    \n",
    "    for tuple in lines:\n",
    "        w_couple = re.split(r'\\t+', tuple.rstrip('\\t'))\n",
    "        print(w_couple)\n",
    "        #! Scommentare per annotazione \"automatica\"\n",
    "        # score = input(\"\\tInserisci il valore di similarità [0 4] per i * \"+ str(w_couple) +\" *:\")\n",
    "        \n",
    "        tsv_writer.writerow(w_couple)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inizio Esercizio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vogliamo trovare il valore massimo di similarità tra due liste di synset, appartenenti a due termini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lista di synset associati ad un termine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(vec1, vec2):\n",
    "    '''\n",
    "    Giben 2 vectors, return the cosine similarity between them\n",
    "    '''\n",
    "    return 1 - spatial.distance.cosine(vec1, vec2)\n",
    "\n",
    "def get_synsets_from_word(path, word):\n",
    "    '''Given the path of the file with the list of synset and a word return the list of synset of the word\n",
    "    Args:\n",
    "        path: path of the file with the list of synset\n",
    "        word: word to search in the file\n",
    "    Returns:\n",
    "        list of synset of the word\n",
    "    '''\n",
    "    syns_list = []\n",
    "    with open(path) as f:\n",
    "        terms_synsets = f.readlines()\n",
    "        for i in range(len(terms_synsets)):\n",
    "            if word in terms_synsets[i]:\n",
    "                index = i\n",
    "                while(terms_synsets[index+1].find('#') == -1):\n",
    "                    syns_list.append(terms_synsets[index+1].rstrip('\\n'))\n",
    "                    index += 1\n",
    "                break\n",
    "    return syns_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_mini_NASARI(path):\n",
    "    '''Given the path of the nasari file it return a dict with the synset as key and the vector as value\n",
    "    Args:\n",
    "        path: path of the nasari file\n",
    "    Returns:\n",
    "        A dict with the synset as key and the vector as value\n",
    "    '''\n",
    "    term_list = {}\n",
    "    with open(path) as fd:\n",
    "        rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "        for row in rd:\n",
    "            synset= row[0]\n",
    "            index = synset.index('_')\n",
    "            synset = synset[:index]\n",
    "            term_list[synset] = row[1:]\n",
    "            \n",
    "    return term_list\n",
    "\n",
    "def max_nasari_similarity(w1, w2):\n",
    "    '''Given 2 word return the max value of similarity based on NASARI vector space model\n",
    "    The method search for synset in nasari file and if find a value evaluate the similarity\n",
    "    Args:\n",
    "        w1: first word\n",
    "        w2: second word\n",
    "    Returns:\n",
    "        max value of similarity\n",
    "    '''\n",
    "    path_synsets = \"../data/es3_res/SemEval17_IT_senses2synsets.txt\"\n",
    "    path_nasari = \"../data/es3_res/mini_NASARI.tsv\"\n",
    "    nasari = parse_mini_NASARI(path_nasari) # Dict with synset as key\n",
    "    synsets_w1 = get_synsets_from_word(path_synsets, w1)\n",
    "    synsets_w2 = get_synsets_from_word(path_synsets, w2)\n",
    "    max_sim = [0, \"\", \"\"]\n",
    "    for s1 in synsets_w1:\n",
    "        if(s1 in nasari.keys()):\n",
    "            for s2 in synsets_w2:\n",
    "                if(s2 in nasari.keys()):\n",
    "                    v1 = list(map(float, nasari[s1]))\n",
    "                    v2 = list(map(float, nasari[s2]))\n",
    "                    sim = cos_sim(v1, v2)\n",
    "                    if sim > max_sim[0]:\n",
    "                        max_sim = [sim, s1, s2]\n",
    "    return max_sim\n",
    "\n",
    "def couple_list(path):\n",
    "    ''' Given a path return a list of couples of terms '''\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        couple_list = []\n",
    "        for couple in lines:\n",
    "            tmp = couple.split(\"\\t\")\n",
    "            tmp[1] = tmp[1].rstrip('\\n')\n",
    "            couple_list.append(tmp)\n",
    "        \n",
    "        return couple_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_list_path = \"resource/term_list.txt\"\n",
    "\n",
    "couples = couple_list(term_list_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between biotopo and biologia is 0.7280350719764522\n",
      "Similarity between magma and vulcano is 0.866337996384421\n",
      "Similarity between brainstorming and telescopio is 0.3177461381124145\n",
      "Similarity between livello and punteggio is 0.9636678559650353\n",
      "Similarity between centesimo and affare is 0.5394193390194117\n",
      "Similarity between partito politico and associazione is 0.7150458360569183\n",
      "Similarity between tsunami and mare is 0.6039623301956638\n",
      "Similarity between struzzo and frutteto is 0.4547827996210163\n",
      "Similarity between cannella and caramella is 0.7942815235847358\n",
      "Similarity between scopa and polvere is 0.7368595348796247\n",
      "Similarity between galassia and astronomo is 0.762448434314831\n",
      "Similarity between succo and frappè is 0.8121781192172036\n",
      "Similarity between tapparella and tenda is 0.7207209826657325\n",
      "Similarity between criminale and colpevole is 0.866294843667411\n",
      "Similarity between cancro al pancreas and chemioterapia is 0.9077949409690469\n",
      "Similarity between passato and antecedente is 0.9124990841848225\n",
      "Similarity between Nazioni Unite and Parlamento Europeo is 0.6628540978996528\n",
      "Similarity between canzone and esecutore is 0.705368002492872\n",
      "Similarity between pistola and taccuino is 0.4688170726670068\n",
      "Similarity between acetilcolina and iride is 0.5836531575476894\n",
      "Similarity between alfabeto and penna is 0.6239118713996581\n",
      "Similarity between coro and cantante is 0.46728424272779456\n",
      "Similarity between legno and coperta is 0.5996375477788698\n",
      "Similarity between soldato and pace is 0.7116712927364236\n",
      "Similarity between inglese and americano is 1\n",
      "Similarity between lucertola and coccodrillo is 0.9383423172673117\n",
      "Similarity between denaro and contante is 0.818203325778631\n",
      "Similarity between Polpo Paul and polpo is 0.8652938820053688\n",
      "Similarity between calendario and vacanza is 0.6878599350179375\n",
      "Similarity between base and sostanza chimica is 0.8683573575238398\n",
      "Similarity between governo and lucertola is 0.3538381429137851\n",
      "Similarity between JPY and riciclaggio di denaro is 0.528934852765649\n",
      "Similarity between DeepMind and Google DeepMind is 0\n",
      "Similarity between scacchi and scacco al re is 0.924531102640724\n",
      "Similarity between sonetto and bellezza is 0.5570418672994323\n",
      "Similarity between lavoratore and ufficio is 0.5948162606704595\n",
      "Similarity between valuta and scambio is 0.7774225000274397\n",
      "Similarity between poliestere and cotone is 0.7683361148349203\n",
      "Similarity between tribunale and giustizia is 0.9382560099320241\n",
      "Similarity between coda and retro is 0.7639288560636074\n",
      "Similarity between incidente and libro is 0.6993008353129027\n",
      "Similarity between groppo and vento is 0.9474495648285739\n",
      "Similarity between piano and armonia is 0.8475030044271167\n",
      "Similarity between sultano and ministero is 0.29830003086996526\n",
      "Similarity between deserto and duna is 0.8643609475273414\n",
      "Similarity between moltiplicazione and divisione is 0.975459725444201\n",
      "Similarity between virus and sangue is 0.6248980971762105\n",
      "Similarity between era glaciale and statuto is 0.3699035459346649\n",
      "Similarity between spada and ambiente is 0.5433485891832435\n",
      "Similarity between scuola elementare and penna is 0.4225190133646528\n"
     ]
    }
   ],
   "source": [
    "for couple in couples:\n",
    "    w1 = couple[0]\n",
    "    w2 = couple[1]\n",
    "    nasari_sim = max_nasari_similarity(w1, w2)\n",
    "    print(\"Similarity between \" + w1 + \" and \" + w2 + \" is \" + str(nasari_sim[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venvradicioni': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe30bb084dadcc2974aabac99d8c6a813637b467837e0e94701709de5a097010"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
