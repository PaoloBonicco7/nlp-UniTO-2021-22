{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizio 3 - SemEval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consegna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parte 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Annotazione:** Annotare il file con i valori di similarità dati da noi e con i `BabelNetSynsetID` scelti da noi dal file `SemEval17_IT_senses2synsets.txt`.\n",
    "   \n",
    "2. **Agreement:** Dopo di che si calcola il `valore medio` di similarità, `Pearson` e `Spearman`\n",
    "   \n",
    "3. **Ricerca Automatica del Synset:** Andiamo a cercare il `BabelNetSynsetID` nel file dato dal prof `mini_NASARI.tsv`\n",
    "   andando a massimizzare la *cosine similarity*. I valori che massimizzeranno quest'ultima saranno i nostri concetti (Synset).\n",
    "\n",
    "   Per far ciò andiamo ad estrarre tutti i synsets relativi ad un termine, dal file `SemEval17_IT_senses2synsets.txt`\n",
    "   e andiamo a calcolare la *cosine similarity* per ogni synset (quelli dei due termini) presente nel file `mini_NASARI.tsv`\n",
    "   e salviamo i due sysnets che massimizzano la *cosine similarity*.\n",
    "\n",
    "4. **Valutazione Similarità:** Sempre con `Pearson` e `Spearman`, tra i nostri valori di similarità e quelli trovati\n",
    "   al passo precedente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Associare i synset ai termini che pensiamo che massimizzino la similarità (fatto a mano) --> Gia fatto al passo 1 \n",
    "\n",
    "2. Output costituito da 6 campi: \n",
    "   1. Term1\n",
    "   2. Term2\n",
    "   3. BS1 (dato da noi)\n",
    "   4. BS2 (dato da noi)\n",
    "   5. Sinonimi Term1 (da synset a lemma)\n",
    "   6. Sinonimi Term2 (da synset a lemma)\n",
    "\n",
    "   <br/>     \n",
    "3. **Agreement dell'anotazione Synset trovati:** Tramite il punteggio `kappa di Cohen` calcoliamo il livello \n",
    "   di agreement (sklearn) tra i synset trovati da noi\n",
    "\n",
    "4. **Valutazione annotazione:** Confronto i nostri concetti (synset) con quelli che massimizzano la `cos_sim`\n",
    "   (Trovati già al passo 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sviluppo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import csv\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1, 2 - Annotazione e Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Organizzazione dei dati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo a selezionare il dati da utilizzare per l'esercizio 3 tramite la funzione data dal professore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = \"../data/es3_res/it.test.data.txt\"\n",
    "annotator = \"Bonicco\"\n",
    "data_file = \"SemEval_data_\" + annotator + \".tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonicco        :\tcoppie nell'intervallo 51-100\n"
     ]
    }
   ],
   "source": [
    "def get_range(surname):\n",
    "    nof_elements = 500\n",
    "    base_idx = (abs(int(hashlib.sha512(surname.encode('utf-8')).hexdigest(), 16)) % 10)\n",
    "    idx_intervallo = base_idx * 50+1\n",
    "    return idx_intervallo\n",
    " \n",
    "\n",
    "input_name = \"Bonicco\"\n",
    "\n",
    "values = []\n",
    "sx = get_range(input_name)\n",
    "values.append(sx)\n",
    "dx = sx+50-1\n",
    "intervallo = \"\" + str(sx) + \"-\" + str(dx)\n",
    "print('{:15}:\\tcoppie nell\\'intervallo {}'.format(input_name, intervallo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creazione file con le coppie di termini che ci interessano - **Non eseguire, file già creato**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['biotopo\\tbiologia', 'magma\\tvulcano', 'brainstorming\\ttelescopio', 'livello\\tpunteggio', 'centesimo\\taffare', 'partito politico\\tassociazione', 'tsunami\\tmare', 'struzzo\\tfrutteto', 'cannella\\tcaramella', 'scopa\\tpolvere', 'galassia\\tastronomo', 'succo\\tfrappè', 'tapparella\\ttenda', 'criminale\\tcolpevole', 'cancro al pancreas\\tchemioterapia', 'passato\\tantecedente', 'Nazioni Unite\\tParlamento Europeo', 'canzone\\tesecutore', 'pistola\\ttaccuino', 'acetilcolina\\tiride', 'alfabeto\\tpenna', 'coro\\tcantante', 'legno\\tcoperta', 'soldato\\tpace', 'inglese\\tamericano', 'lucertola\\tcoccodrillo', 'denaro\\tcontante', 'Polpo Paul\\tpolpo', 'calendario\\tvacanza', 'base\\tsostanza chimica', 'governo\\tlucertola', 'JPY\\triciclaggio di denaro', 'DeepMind\\tGoogle DeepMind', 'scacchi\\tscacco al re', 'sonetto\\tbellezza', 'lavoratore\\tufficio', 'valuta\\tscambio', 'poliestere\\tcotone', 'tribunale\\tgiustizia', 'coda\\tretro', 'incidente\\tlibro', 'groppo\\tvento', 'piano\\tarmonia', 'sultano\\tministero', 'deserto\\tduna', 'moltiplicazione\\tdivisione', 'virus\\tsangue', 'era glaciale\\tstatuto', 'spada\\tambiente', 'scuola elementare\\tpenna']\n"
     ]
    }
   ],
   "source": [
    "with open(path_data) as file:\n",
    "    lines = file.readlines()[50:100]\n",
    "    lines = [line.rstrip() for line in lines]\n",
    "    print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['biotopo', 'biologia']\n",
      "['magma', 'vulcano']\n",
      "['brainstorming', 'telescopio']\n",
      "['livello', 'punteggio']\n",
      "['centesimo', 'affare']\n",
      "['partito politico', 'associazione']\n",
      "['tsunami', 'mare']\n",
      "['struzzo', 'frutteto']\n",
      "['cannella', 'caramella']\n",
      "['scopa', 'polvere']\n",
      "['galassia', 'astronomo']\n",
      "['succo', 'frappè']\n",
      "['tapparella', 'tenda']\n",
      "['criminale', 'colpevole']\n",
      "['cancro al pancreas', 'chemioterapia']\n",
      "['passato', 'antecedente']\n",
      "['Nazioni Unite', 'Parlamento Europeo']\n",
      "['canzone', 'esecutore']\n",
      "['pistola', 'taccuino']\n",
      "['acetilcolina', 'iride']\n",
      "['alfabeto', 'penna']\n",
      "['coro', 'cantante']\n",
      "['legno', 'coperta']\n",
      "['soldato', 'pace']\n",
      "['inglese', 'americano']\n",
      "['lucertola', 'coccodrillo']\n",
      "['denaro', 'contante']\n",
      "['Polpo Paul', 'polpo']\n",
      "['calendario', 'vacanza']\n",
      "['base', 'sostanza chimica']\n",
      "['governo', 'lucertola']\n",
      "['JPY', 'riciclaggio di denaro']\n",
      "['DeepMind', 'Google DeepMind']\n",
      "['scacchi', 'scacco al re']\n",
      "['sonetto', 'bellezza']\n",
      "['lavoratore', 'ufficio']\n",
      "['valuta', 'scambio']\n",
      "['poliestere', 'cotone']\n",
      "['tribunale', 'giustizia']\n",
      "['coda', 'retro']\n",
      "['incidente', 'libro']\n",
      "['groppo', 'vento']\n",
      "['piano', 'armonia']\n",
      "['sultano', 'ministero']\n",
      "['deserto', 'duna']\n",
      "['moltiplicazione', 'divisione']\n",
      "['virus', 'sangue']\n",
      "['era glaciale', 'statuto']\n",
      "['spada', 'ambiente']\n",
      "['scuola elementare', 'penna']\n"
     ]
    }
   ],
   "source": [
    "terms_sim_score = []\n",
    "\n",
    "with open(data_file, 'wt') as out_file:\n",
    "    tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "    tsv_writer.writerow([\"Term1\", \"Term2\", \"Similarity\"])\n",
    "    \n",
    "    for tuple in lines:\n",
    "        w_couple = re.split(r'\\t+', tuple.rstrip('\\t'))\n",
    "        print(w_couple)\n",
    "        #! Scommentare per annotazione \"automatica\"\n",
    "        # score = input(\"\\tInserisci il valore di similarità [0 4] per i * \"+ str(w_couple) +\" *:\")\n",
    "        \n",
    "        tsv_writer.writerow(w_couple)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel seguente file excel abbiamo annotato i punteggi di similarità relativi alla coppia di termini, dopo di che abbiamo calcolato la media tra i valori dati\n",
    "dai due annotatori, oltre ai due indici di correlazione *Pearson* e *Spearman*.\n",
    "\n",
    "Inoltre, abbiamo anche annotato i synset relativi ai termini, andandoli a scegliere tra la lista dei synset data dal professore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://1drv.ms/x/s!AkdbdBkdl_1dhbcgYl1tXDNpjknAIw?e=UYsHF5\"> Link all'excel con i dati </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calcolo Spearman per Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_index_np(x, y): # [3,45 ; 5,66 ; ..]\n",
    "    '''\n",
    "    Implementation of the Pearson index using numpy\n",
    "    Args:\n",
    "         x: golden value\n",
    "         y: similarity list\n",
    "    Returns:\n",
    "        Pearson correlation index = [Covariance / (Standard deviation of x * Standard deviation of y)]\n",
    "    '''\n",
    "    mu_x = np.mean(x)\n",
    "    mu_y = np.mean(y)\n",
    "    \n",
    "    # and the standard deviation of both\n",
    "    std_dev_x = np.std(x)\n",
    "    std_dev_y = np.std(y)\n",
    "    \n",
    "    numeric = std_dev_x * std_dev_y\n",
    "\n",
    "    num = np.cov(x, y)[0][1] # Covariance    \n",
    "\n",
    "    return num / numeric\n",
    "\n",
    "def spearman_index(x, y):\n",
    "    '''\n",
    "    Implementation of the Spearman index.\n",
    "    Args:\n",
    "        x: golden value\n",
    "        y: similarity list\n",
    "    Returns:\n",
    "         Spearman correlation index\n",
    "    '''\n",
    "    rank__x = define_rank(x)\n",
    "    rank__y = define_rank(y)\n",
    "\n",
    "    return pearson_index_np(rank__x, rank__y)\n",
    "\n",
    "\n",
    "def define_rank(vector):\n",
    "    '''\n",
    "    Args:\n",
    "        vector: numeric vector\n",
    "    Returns:\n",
    "        ranks list, sorted as the input order\n",
    "    '''\n",
    "    x_couple = [(vector[i], i) for i in range(len(vector))]\n",
    "    x_couple_sorted = sorted(x_couple, key=lambda x: x[0])\n",
    "\n",
    "    return [y for (x, y) in x_couple_sorted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "sim_paolo = [2,2.6,0,1.8,0.8,1.5,3,0,1,2.5,2.8,1,2.4,2.8,2.2,4,1.4,1.6,0,0.2,0.5,3,0,1.8,2,2.6,3,3,1,3,0,1.4,4,3.6,0.4,3,2.5,2,2.8,0.4,0,3,2.8,1.8,3.4,3.6,1.8,0,0,2.4]\n",
    "sim_davide = [1.8,2,0,1.8,1.2,2.8,1,0,0.8,1,1,3,1.5,2.8,1.8,3.5,0.8,0,0,0.5,1,3,0.8,1,3.2,3,4,2.5,1,2,0,1.2,4,2,0,2,2,3,2,3.8,0,2,1.5,1.2,2,3,1.5,0,0,1.2]\n",
    "\n",
    "print(len(sim_paolo) == len(sim_davide) == 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1991817135017272"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearman_index(sim_paolo, sim_davide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open(data_file, 'w').close() # Puliamo il file con i termini di riferimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Ricerca automatica del Synset - Massimizzazione cosine similarity da mini_NASARI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lista di synset associati ad un termine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(vec1, vec2):\n",
    "    '''\n",
    "    Given 2 vectors, return the cosine similarity between them\n",
    "    '''\n",
    "    return 1 - spatial.distance.cosine(vec1, vec2)\n",
    "\n",
    "def get_synsets_from_word(path, word):\n",
    "    '''Given the path of the file with the list of synset and a word return the list of synset of the word\n",
    "    Args:\n",
    "        path: path of the file with the list of synset\n",
    "        word: word to search in the file\n",
    "    Returns:\n",
    "        list of synset of the word\n",
    "    '''\n",
    "    syns_list = []\n",
    "    with open(path) as f:\n",
    "        terms_synsets = f.readlines()\n",
    "        for i in range(len(terms_synsets)):\n",
    "            if word in terms_synsets[i]:\n",
    "                index = i\n",
    "                while(terms_synsets[index+1].find('#') == -1): # and index < len(terms_synsets)): #! Funziona anche senza perchè non andiamo a prendere l'ultimo termine\n",
    "                    syns_list.append(terms_synsets[index+1].rstrip('\\n'))\n",
    "                    index += 1\n",
    "                break\n",
    "    return syns_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_mini_NASARI(path):\n",
    "    '''Given the path of the nasari file it return a dict with the synset as key and the vector as value\n",
    "    Args:\n",
    "        path: path of the nasari file\n",
    "    Returns:\n",
    "        A dict with the synset as key and the vector as value\n",
    "    '''\n",
    "    term_list = {}\n",
    "    with open(path) as fd:\n",
    "        rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "        for row in rd:\n",
    "            synset= row[0]\n",
    "            index = synset.index('_')\n",
    "            synset = synset[:index]\n",
    "            term_list[synset] = row[1:]\n",
    "            \n",
    "    return term_list\n",
    "\n",
    "def max_nasari_similarity(w1, w2):\n",
    "    '''Given 2 word return the max value of similarity based on NASARI vector space model\n",
    "    The method search for synset in nasari file and if find a value evaluate the similarity\n",
    "    Args:\n",
    "        w1: first word\n",
    "        w2: second word\n",
    "    Returns:\n",
    "        max value of similarity\n",
    "    '''\n",
    "    path_synsets = \"../data/es3_res/SemEval17_IT_senses2synsets.txt\"\n",
    "    path_nasari = \"../data/es3_res/mini_NASARI.tsv\"\n",
    "    nasari = parse_mini_NASARI(path_nasari) # Dict with synset as key\n",
    "    synsets_w1 = get_synsets_from_word(path_synsets, w1)\n",
    "    synsets_w2 = get_synsets_from_word(path_synsets, w2)\n",
    "    max_sim = [0, \"\", \"\"]\n",
    "    for s1 in synsets_w1:\n",
    "        if(s1 in nasari.keys()):\n",
    "            for s2 in synsets_w2:\n",
    "                if(s2 in nasari.keys()):\n",
    "                    v1 = list(map(float, nasari[s1]))\n",
    "                    v2 = list(map(float, nasari[s2]))\n",
    "                    sim = cos_sim(v1, v2)\n",
    "                    if sim > max_sim[0]:\n",
    "                        max_sim = [sim, s1, s2]\n",
    "    return max_sim\n",
    "\n",
    "def couple_list(path):\n",
    "    ''' Given a path return a list of couples of terms '''\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        couple_list = []\n",
    "        for couple in lines:\n",
    "            tmp = couple.split(\"\\t\")\n",
    "            tmp[1] = tmp[1].rstrip('\\n')\n",
    "            couple_list.append(tmp)\n",
    "        \n",
    "        return couple_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_list_path = \"resource/term_list.txt\"\n",
    "\n",
    "couples = couple_list(term_list_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "for couple in couples:\n",
    "    w1 = couple[0]\n",
    "    w2 = couple[1]\n",
    "    nasari_sim = max_nasari_similarity(w1, w2)\n",
    "    # print(\"Similarity between \" + w1 + \" and \" + w2 + \" is \" + str(nasari_sim[0]))\n",
    "    # print(round(nasari_sim[0], 4))\n",
    "    # print(nasari_sim[1] + \"\\t\" + nasari_sim[2])\n",
    "    values.append(round(nasari_sim[0], 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spearman tra media similarità annotatori e cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.137"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim = values\n",
    "sim_mean = [1.9, 2.3, 0, 1.8, 1, 2.15, 2, 0, 0.9, 1.75, 1.9, 2, 1.95, 2.8, 2, 3.75, 1.1, 0.8, 0, 0.35, 0.75, 3, 0.4, 1.4, 2.6, 2.8, 3.5, 2.75, 1, 2.5, 0, 1.3, 4, 2.8, 0.2, 2.5, 2.25, 2.5, 2.4, 2.1, 0, 2.5, 2.15,1.5, 2.7, 3.3, 1.65, 0, 0, 1.8]\n",
    "\n",
    "round(spearman_index(cos_sim, sim_mean), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KEY = '8be6bdbd-78ee-4efb-91a2-3854f79a97e0'\n",
    "KEY = '4ac4317e-cf96-4222-ba17-753d12dc7a2f'\n",
    "URL = 'https://babelnet.io/v7/getSynset?id={}&key={}&targetLang=IT&searchLang=IT'\n",
    "    \n",
    "def get_synonims_for_synsets(babel_synsets_ids):\n",
    "    '''\n",
    "    Given a list of babelNetSynset IDs returns a dictionary containing,\n",
    "    for each ID, the set of all lemmas in the corresponding BabelNet Synset.\n",
    "    '''\n",
    "    res = dict()\n",
    "    for id in babel_synsets_ids:\n",
    "        res[id] = set()\n",
    "        x = requests.get(URL.format(id,KEY))\n",
    "        if x.status_code == 400:\n",
    "            continue\n",
    "        senses = x.json()['senses']\n",
    "        for sense in senses:\n",
    "            res[id].add(sense['properties']['fullLemma'])\n",
    "    return res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venvradicioni': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe30bb084dadcc2974aabac99d8c6a813637b467837e0e94701709de5a097010"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
