{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Es2 - Word Sense Disambiguation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo esercizio andremo ad estrarre 50 frasi causuali dal corpus `SemCor` e proveremo a disambiguare un sostantivo per ogni frase, anche\n",
    "quest'ultimo estratto casualmente dalla frase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Estrazione casuale delle frasi dal corpus `SemCor`\n",
    "2. Pulizia delle frasi:\n",
    "   1. Rimozione stopwords, punteggiatura e lemming\n",
    "3. Estrazione di un sostantivo casuale dalla frase\n",
    "4. Estrazione dei synset del sostantivo **?? (domanda, estraggo solo i sysnet che sono etichettati come *NN*)**\n",
    "5. Costruzione della `Bag of Words` per la frase e del sostantivo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparazione dei dati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and dataset downlaod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import semcor\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import random\n",
    "from pprint import pprint\n",
    "from nltk.tree import *\n",
    "\n",
    "# nltk.download('semcor') # download the semcor corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrazione frasi da corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "sents = semcor.sents()\n",
    "\n",
    "sents_full = semcor.tagged_sents(tag=\"both\")\n",
    "\n",
    "print(len(sents) == len(sents_full)) # True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metodi utili per gestione Corpus `SemCor` e struttura `Tree` di `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma(word):\n",
    "    '''\n",
    "    Args:\n",
    "        word: term as as nltk.Tree\n",
    "    Returns:\n",
    "        lemma of the word as a string.\n",
    "        If there isn't a lemma, return the PoS tag or None.\n",
    "    '''\n",
    "    return word.label()\n",
    "\n",
    "def get_word(word):\n",
    "    '''\n",
    "    Args:\n",
    "        word: term as as nltk.tree.tree.Tree\n",
    "    Returns:\n",
    "        the term as as a list of strings.\n",
    "        Return a list because a term may consist of several words\n",
    "    '''\n",
    "    if(isinstance(word, nltk.Tree)):\n",
    "        return word.leaves()\n",
    "    return None\n",
    "\n",
    "def get_pos(word):\n",
    "    '''\n",
    "    Args:\n",
    "        word: term as as nltk.Tree\n",
    "    Returns:\n",
    "        The PoS tag of a word or None if there is no PoS for the term \n",
    "        (es. '!' has tag None)\n",
    "    '''\n",
    "    return word.pos()[0][1]\n",
    "\n",
    "def get_synset(lemma):\n",
    "    '''\n",
    "    Args:\n",
    "        lemma: Lemma of a word\n",
    "    Returns:\n",
    "        The synset associated to the lemma\n",
    "    '''\n",
    "    if(isinstance(lemma, nltk.corpus.reader.wordnet.Lemma)):\n",
    "        return lemma.synset()\n",
    "    return None\n",
    "\n",
    "def get_sents(semcor):\n",
    "    '''\n",
    "    Args:\n",
    "        semcor: Semcor corpus\n",
    "    Return:\n",
    "        a list of list of words. Each list of words is a sentence.\n",
    "    '''\n",
    "    return semcor.sents()\n",
    "\n",
    "def get_term(lemma):\n",
    "    '''\n",
    "    Args:\n",
    "        lemma: Lemma of a word\n",
    "    Returns:\n",
    "        The term associated to the lemma\n",
    "    '''\n",
    "    if(isinstance(lemma, nltk.corpus.reader.wordnet.Lemma)):\n",
    "        return lemma.name()\n",
    "    return None\n",
    "\n",
    "def get_synsets(term):\n",
    "    '''\n",
    "    Retrurn the synsets of a term.\n",
    "    '''\n",
    "    if(len(wn.synsets(term)) > 0):\n",
    "        return wn.synsets(term)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('primary.n.01')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma = \"primary_election.n.01.primary\"\n",
    "wn.lemma(lemma).synset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selezione delle frasi casuali"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estraiamo le frasi come stringhe e come oggetti `Tree` per poter ottenere anche il pos e il \n",
    "lemma associato ad un termine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sent(sent):\n",
    "    '''\n",
    "        Check if there is a NN with his lemma in the sentence and that as more than\n",
    "        1 synset, so that the term is ambiguous\n",
    "    '''\n",
    "    for el in sent:\n",
    "        if(get_pos(el) == 'NN'):\n",
    "            lemma = get_lemma(el)\n",
    "            if(isinstance(lemma, nltk.corpus.reader.wordnet.Lemma)):\n",
    "                term = get_term(lemma)\n",
    "                syns = wn.synsets(term)\n",
    "                if(len(syns) > 1):\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "def pick_sents(s, sfull, num):\n",
    "    rand_sents, rand_num, rand_full_sent = [], [], []\n",
    "    l = len(s) - 1\n",
    "    n = random.randint(0, l)\n",
    "    \n",
    "    while (len(rand_sents) < num):\n",
    "        while(n in rand_num):\n",
    "            n = random.randint(0, l)\n",
    "            \n",
    "        rand_num.append(n)\n",
    "        \n",
    "        if(check_sent(sfull[n])):\n",
    "            rand_sents.append(s[n])\n",
    "            rand_full_sent.append(sfull[n])\n",
    "    \n",
    "    return rand_sents, rand_full_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estraggo 50 frasi casuali dal corpus `SemCor`\n",
    "\n",
    "Ottengo una lista di oggetti *semcor* -> `nltk.corpus.reader.semcor.SemcorSentence`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_list, sent_full_list = pick_sents(sents, sents_full, 50)\n",
    "\n",
    "# print(len(sent_list) == len(sent_full_list)) # 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementazione algoritmo di Lesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils.log import error\n",
    "\n",
    "def find_noun(sent):\n",
    "    '''\n",
    "    Take a sentence and return a random Noun in the phrase with his right synset associated\n",
    "    '''\n",
    "    noun_list = []\n",
    "    for el in sent:\n",
    "        if(get_pos(el) == 'NN'):\n",
    "            lemma = get_lemma(el)\n",
    "            if(isinstance(lemma, nltk.corpus.reader.wordnet.Lemma)):\n",
    "                term = get_term(lemma)\n",
    "                syns = wn.synsets(term)\n",
    "                if(len(syns) > 1):\n",
    "                    noun_list.append(el)\n",
    "    return noun_list[random.randint(0, len(noun_list) - 1)]\n",
    "\n",
    "\n",
    "def bag_of_word(sent):\n",
    "    '''\n",
    "    Auxiliary function for the Lesk algorithm. Transforms the given sentence\n",
    "    according to the bag of words approach, apply lemmatization, stop words\n",
    "    and punctuation removal.\n",
    "    Args:\n",
    "        sent: sentence\n",
    "    Returns:\n",
    "        bag of words\n",
    "    '''\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = {',', ';', '(', ')', '{', '}', ':', '?', '!', '.', '``', '*', '-'}\n",
    "    # Returns the input word unchanged if it cannot be found in WordNet.\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    # Return a tokenized copy of text, using NLTKâ€™s recommended word tokenizer (Treebank + PunkSentence)\n",
    "    tokens = nltk.word_tokenize(sent)\n",
    "    tokens = list(filter(lambda x: x not in stop_words and x not in punctuation, tokens))\n",
    "    return list(wnl.lemmatize(t.lower()) for t in tokens)\n",
    "\n",
    "\n",
    "def get_context(sent):\n",
    "    '''\n",
    "    Auxiliary function for the Lesk algorithm. Returns the context of the given word in the given sentence.\n",
    "    Args:\n",
    "        sent: sentence\n",
    "    Returns:\n",
    "        set of words in the sentence after stop words and punctuation removal and lemming\n",
    "    '''\n",
    "    context = []\n",
    "    merged_sent = ' '.join(word for word in sent)\n",
    "    context.append(bag_of_word(merged_sent))\n",
    "    return context[0]\n",
    "\n",
    "\n",
    "def get_signature(syn):\n",
    "    '''\n",
    "    Args:\n",
    "        synset: a synset of a word\n",
    "    Returns:\n",
    "        A list of word formed by examples and gloss of the synset\n",
    "    '''\n",
    "    bof = bag_of_word(syn.definition())\n",
    "    for el in syn.examples():\n",
    "        bof.extend(bag_of_word(el))\n",
    "    return bof\n",
    "\n",
    "\n",
    "def get_overlap(s1, s2):\n",
    "    '''\n",
    "    Args:\n",
    "        s1: list of words\n",
    "        s2: list of words\n",
    "    Returns:\n",
    "        The number of words in s1 that are also in s2\n",
    "    '''\n",
    "    return len(set(s1).intersection(set(s2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lesk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `Contesto` - Insieme delle parole presenti nella frase\n",
    "2. `Signature` - Insieme della parole presenti nella definizione e negli esempi dei synset del termine da disambiguare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lesk(word, sentence):\n",
    "    max_overlap = 0\n",
    "    best_synset = None\n",
    "    context = get_context(sentence)\n",
    "    word_synsets = wn.synsets(word)\n",
    "    for syn in word_synsets:\n",
    "        signature = get_signature(syn)\n",
    "        overlap = get_overlap(signature, context)\n",
    "        if(overlap > max_overlap):\n",
    "            max_overlap = overlap\n",
    "            best_synset = syn\n",
    "    return best_synset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterazione di prova su una sola frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_list, sent_full_list = pick_sents(sents, sents_full, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word = find_noun(sent_full_list[0])\n",
    "lemma = get_lemma(target_word)\n",
    "target_word_str = get_term(lemma)\n",
    "target_synset = lemma.synset()\n",
    "\n",
    "sent_0 = sent_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Word: future\n",
      "\n",
      "Sentence: It is on them alone that the future of their race depends , for all their relatives ( mothers , husbands , brothers , and unmated sisters ) have perished with the arrival of the cold weather .\n",
      "\n",
      "Result: future.n.02 -- Correct: future.n.01\n"
     ]
    }
   ],
   "source": [
    "print(\"Target Word: \" + target_word_str)\n",
    "print(\"\\nSentence: \" + ' '.join(sent_0))\n",
    "\n",
    "lesk_syns = lesk(target_word_str, sent_0)\n",
    "\n",
    "print(\"\\nResult: \" + lesk_syns.name(), \"-- Correct: \" + target_synset.name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prova su 50 frasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venvradicioni': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe30bb084dadcc2974aabac99d8c6a813637b467837e0e94701709de5a097010"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
