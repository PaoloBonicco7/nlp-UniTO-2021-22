{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizio 3 - SemEval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consegna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parte 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Annotazione:** Annotare il file con i valori di similarità dati da noi e con i `BabelNetSynsetID` scelti da noi dal file `SemEval17_IT_senses2synsets.txt`.\n",
    "   \n",
    "2. **Agreement:** Dopo di che si calcola il `valore medio` di similarità, `Pearson` e `Spearman`\n",
    "   \n",
    "3. **Ricerca Automatica del Synset:** Andiamo a cercare il `BabelNetSynsetID` nel file dato dal prof `mini_NASARI.tsv`\n",
    "   andando a massimizzare la *cosine similarity*. I valori che massimizzeranno quest'ultima saranno i nostri concetti (Synset).\n",
    "\n",
    "   Per far ciò andiamo ad estrarre tutti i synsets relativi ad un termine, dal file `SemEval17_IT_senses2synsets.txt`\n",
    "   e andiamo a calcolare la *cosine similarity* per ogni synset (quelli dei due termini) presente nel file `mini_NASARI.tsv`\n",
    "   e salviamo i due sysnets che massimizzano la *cosine similarity*.\n",
    "\n",
    "4. **Valutazione Similarità:** Sempre con `Pearson` e `Spearman`, tra i nostri valori di similarità e quelli trovati\n",
    "   al passo precedente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Associare i synset ai termini che pensiamo che massimizzino la similarità (fatto a mano) --> Gia fatto al passo 1 \n",
    "\n",
    "2. Output costituito da 6 campi: \n",
    "   1. Term1\n",
    "   2. Term2\n",
    "   3. BS1 (dato da noi)\n",
    "   4. BS2 (dato da noi)\n",
    "   5. Sinonimi Term1 (da synset a lemma)\n",
    "   6. Sinonimi Term2 (da synset a lemma)\n",
    "\n",
    "   <br/>     \n",
    "3. **Agreement dell'anotazione Synset trovati:** Tramite il punteggio `kappa di Cohen` calcoliamo il livello \n",
    "   di agreement (sklearn) tra i synset trovati da noi\n",
    "\n",
    "4. **Valutazione annotazione:** Confronto i nostri concetti (synset) con quelli che massimizzano la `cos_sim`\n",
    "   (Trovati già al passo 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sviluppo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import csv\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1, 2 - Annotazione e Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Organizzazione dei dati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo a selezionare il dati da utilizzare per l'esercizio 3 tramite la funzione data dal professore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = \"../data/es3_res/it.test.data.txt\"\n",
    "annotator = \"Bonicco\"\n",
    "data_file = \"SemEval_data_\" + annotator + \".tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codice del prof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonicco        :\tcoppie nell'intervallo 51-100\n"
     ]
    }
   ],
   "source": [
    "def get_range(surname):\n",
    "    nof_elements = 500\n",
    "    base_idx = (abs(int(hashlib.sha512(surname.encode('utf-8')).hexdigest(), 16)) % 10)\n",
    "    idx_intervallo = base_idx * 50+1\n",
    "    return idx_intervallo\n",
    " \n",
    "\n",
    "input_name = \"Bonicco\"\n",
    "\n",
    "values = []\n",
    "sx = get_range(input_name)\n",
    "values.append(sx)\n",
    "dx = sx+50-1\n",
    "intervallo = \"\" + str(sx) + \"-\" + str(dx)\n",
    "print('{:15}:\\tcoppie nell\\'intervallo {}'.format(input_name, intervallo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creazione file con le coppie di termini che ci interessano - **Non eseguire, file già creato**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['biotopo\\tbiologia', 'magma\\tvulcano', 'brainstorming\\ttelescopio', 'livello\\tpunteggio', 'centesimo\\taffare', 'partito politico\\tassociazione', 'tsunami\\tmare', 'struzzo\\tfrutteto', 'cannella\\tcaramella', 'scopa\\tpolvere', 'galassia\\tastronomo', 'succo\\tfrappè', 'tapparella\\ttenda', 'criminale\\tcolpevole', 'cancro al pancreas\\tchemioterapia', 'passato\\tantecedente', 'Nazioni Unite\\tParlamento Europeo', 'canzone\\tesecutore', 'pistola\\ttaccuino', 'acetilcolina\\tiride', 'alfabeto\\tpenna', 'coro\\tcantante', 'legno\\tcoperta', 'soldato\\tpace', 'inglese\\tamericano', 'lucertola\\tcoccodrillo', 'denaro\\tcontante', 'Polpo Paul\\tpolpo', 'calendario\\tvacanza', 'base\\tsostanza chimica', 'governo\\tlucertola', 'JPY\\triciclaggio di denaro', 'DeepMind\\tGoogle DeepMind', 'scacchi\\tscacco al re', 'sonetto\\tbellezza', 'lavoratore\\tufficio', 'valuta\\tscambio', 'poliestere\\tcotone', 'tribunale\\tgiustizia', 'coda\\tretro', 'incidente\\tlibro', 'groppo\\tvento', 'piano\\tarmonia', 'sultano\\tministero', 'deserto\\tduna', 'moltiplicazione\\tdivisione', 'virus\\tsangue', 'era glaciale\\tstatuto', 'spada\\tambiente', 'scuola elementare\\tpenna']\n"
     ]
    }
   ],
   "source": [
    "with open(path_data) as file:\n",
    "    lines = file.readlines()[50:100]\n",
    "    lines = [line.rstrip() for line in lines]\n",
    "    print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'terms_sim_score = []\\n\\n with open(data_file, \\'wt\\') as out_file:\\n    tsv_writer = csv.writer(out_file, delimiter=\\'\\t\\')\\n    tsv_writer.writerow([\"Term1\", \"Term2\", \"Similarity\"])\\n    \\n    for tuple in lines:\\n        w_couple = re.split(r\\'\\t+\\', tuple.rstrip(\\'\\t\\'))\\n        print(w_couple)\\n        #! Scommentare per annotazione \"automatica\"\\n        # score = input(\"\\tInserisci il valore di similarità [0 4] per i * \"+ str(w_couple) +\" *:\")\\n        \\n        tsv_writer.writerow(w_couple) '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"terms_sim_score = []\n",
    "\n",
    " with open(data_file, 'wt') as out_file:\n",
    "    tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "    tsv_writer.writerow([\"Term1\", \"Term2\", \"Similarity\"])\n",
    "    \n",
    "    for tuple in lines:\n",
    "        w_couple = re.split(r'\\t+', tuple.rstrip('\\t'))\n",
    "        print(w_couple)\n",
    "        #! Scommentare per annotazione \"automatica\"\n",
    "        # score = input(\"\\tInserisci il valore di similarità [0 4] per i * \"+ str(w_couple) +\" *:\")\n",
    "        \n",
    "        tsv_writer.writerow(w_couple) \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel seguente file excel abbiamo annotato i punteggi di similarità relativi alla coppia di termini, dopo di che abbiamo calcolato la media tra i valori dati\n",
    "dai due annotatori, oltre ai due indici di correlazione *Pearson* e *Spearman*.\n",
    "\n",
    "Inoltre, abbiamo anche annotato i synset relativi ai termini, andandoli a scegliere tra la lista dei synset data dal professore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://1drv.ms/x/s!AkdbdBkdl_1dhbcgYl1tXDNpjknAIw?e=UYsHF5\"> Link all'excel con i dati </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calcolo Spearman per Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_index_np(x, y): # [3,45 ; 5,66 ; ..]\n",
    "    '''\n",
    "    Implementation of the Pearson index using numpy\n",
    "    Args:\n",
    "         x: golden value\n",
    "         y: similarity list\n",
    "    Returns:\n",
    "        Pearson correlation index = [Covariance / (Standard deviation of x * Standard deviation of y)]\n",
    "    '''\n",
    "    mu_x = np.mean(x)\n",
    "    mu_y = np.mean(y)\n",
    "    \n",
    "    # and the standard deviation of both\n",
    "    std_dev_x = np.std(x)\n",
    "    std_dev_y = np.std(y)\n",
    "    \n",
    "    numeric = std_dev_x * std_dev_y\n",
    "\n",
    "    num = np.cov(x, y)[0][1] # Covariance    \n",
    "\n",
    "    return num / numeric\n",
    "\n",
    "def spearman_index(x, y):\n",
    "    '''\n",
    "    Implementation of the Spearman index.\n",
    "    Args:\n",
    "        x: golden value\n",
    "        y: similarity list\n",
    "    Returns:\n",
    "         Spearman correlation index\n",
    "    '''\n",
    "    rank__x = define_rank(x)\n",
    "    rank__y = define_rank(y)\n",
    "\n",
    "    return pearson_index_np(rank__x, rank__y)\n",
    "\n",
    "def spearman_index_scipy(x, y):\n",
    "    '''\n",
    "    Implementation of the Spearman index using scipy\n",
    "    '''\n",
    "    return stats.spearmanr(x, y)[0]\n",
    "\n",
    "def pearson_index_scipy(x, y):\n",
    "    '''\n",
    "    Implementation of the Pearson index using scipy\n",
    "    Args:\n",
    "         x: golden value\n",
    "         y: similarity list\n",
    "    Returns:\n",
    "        Pearson correlation index = [Covariance / (Standard deviation of x * Standard deviation of y)]\n",
    "    '''\n",
    "    return stats.pearsonr(x, y)[0]\n",
    "\n",
    "def define_rank(vector):\n",
    "    '''\n",
    "    Args:\n",
    "        vector: numeric vector\n",
    "    Returns:\n",
    "        ranks list, sorted as the input order\n",
    "    '''\n",
    "    x_couple = [(vector[i], i) for i in range(len(vector))]\n",
    "    x_couple_sorted = sorted(x_couple, key=lambda x: x[0])\n",
    "\n",
    "    return [y for (x, y) in x_couple_sorted]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I valori di similarità dati dai due annotatori, Paolo e Davide, sono contenuti nelle seguenti liste e sono stati annotati sul file excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "sim_paolo = [2,2.6,0,1.8,0.8,1.5,3,0,1,2.5,2.8,1,2.4,2.8,2.2,4,1.4,1.6,0,0.2,0.5,3,0,1.8,2,2.6,3,3,1,3,0,1.4,4,3.6,0.4,3,2.5,2,2.8,0.4,0,3,2.8,1.8,3.4,3.6,1.8,0,0,2.4]\n",
    "\n",
    "sim_davide = [1.8,2,0,1.8,1.2,2.8,1,0,0.8,1,1,3,1.5,2.8,1.8,3.5,0.8,0,0,0.5,1,3,0.8,1,3.2,3,4,2.5,1,2,0,1.2,4,2,0,2,2,3,2,3.8,0,2,1.5,1.2,2,3,1.5,0,0,1.2]\n",
    "\n",
    "print(len(sim_paolo) == len(sim_davide) == 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6647"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(pearson_index_scipy(sim_paolo, sim_davide), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6647"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(pearson_index_scipy(sim_paolo, sim_davide), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open(data_file, 'w').close() # Puliamo il file con i termini di riferimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Ricerca automatica del Synset - Massimizzazione cosine similarity da mini_NASARI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lista di synset associati ad un termine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(vec1, vec2):\n",
    "    '''\n",
    "    Given 2 vectors, return the cosine similarity between them\n",
    "    '''\n",
    "    return 1 - spatial.distance.cosine(vec1, vec2)\n",
    "\n",
    "def get_synsets_from_word(path, word):\n",
    "    '''Given the path of the file with the list of synset and a word return the list of synset of the word\n",
    "    Args:\n",
    "        path: path of the file with the list of synset\n",
    "        word: word to search in the file\n",
    "    Returns:\n",
    "        list of synset of the word\n",
    "    '''\n",
    "    syns_list = []\n",
    "    with open(path) as f:\n",
    "        terms_synsets = f.readlines()\n",
    "        for i in range(len(terms_synsets)):\n",
    "            if word in terms_synsets[i]:\n",
    "                index = i\n",
    "                while(terms_synsets[index+1].find('#') == -1): # and index < len(terms_synsets)): #! Funziona anche senza perchè non andiamo a prendere l'ultimo termine\n",
    "                    syns_list.append(terms_synsets[index+1].rstrip('\\n'))\n",
    "                    index += 1\n",
    "                break\n",
    "    return syns_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_mini_NASARI(path):\n",
    "    '''Given the path of the nasari file it return a dict with the synset as key and the vector as value\n",
    "    Args:\n",
    "        path: path of the nasari file\n",
    "    Returns:\n",
    "        A dict with the synset as key and the vector as value\n",
    "    '''\n",
    "    term_list = {}\n",
    "    with open(path) as fd:\n",
    "        rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "        for row in rd:\n",
    "            synset= row[0]\n",
    "            index = synset.index('_')\n",
    "            synset = synset[:index]\n",
    "            term_list[synset] = row[1:]\n",
    "            \n",
    "    return term_list\n",
    "\n",
    "def max_nasari_similarity(w1, w2):\n",
    "    '''Given 2 word return the max value of similarity based on NASARI vector space model\n",
    "    The method search for synset in nasari file and if find a value evaluate the similarity\n",
    "    Args:\n",
    "        w1: first word\n",
    "        w2: second word\n",
    "    Returns:\n",
    "        max value of similarity\n",
    "    '''\n",
    "    path_synsets = \"../data/es3_res/SemEval17_IT_senses2synsets.txt\"\n",
    "    path_nasari = \"../data/es3_res/mini_NASARI.tsv\"\n",
    "    nasari = parse_mini_NASARI(path_nasari) # Dict with synset as key\n",
    "    synsets_w1 = get_synsets_from_word(path_synsets, w1)\n",
    "    synsets_w2 = get_synsets_from_word(path_synsets, w2)\n",
    "    max_sim = [0, \"\", \"\"]\n",
    "    for s1 in synsets_w1:\n",
    "        if(s1 in nasari.keys()):\n",
    "            for s2 in synsets_w2:\n",
    "                if(s2 in nasari.keys()):\n",
    "                    v1 = list(map(float, nasari[s1]))\n",
    "                    v2 = list(map(float, nasari[s2]))\n",
    "                    sim = cos_sim(v1, v2)\n",
    "                    if sim > max_sim[0]:\n",
    "                        max_sim = [sim, s1, s2]\n",
    "    return max_sim\n",
    "\n",
    "def couple_list(path):\n",
    "    ''' Given a path return a list of couples of terms '''\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        couple_list = []\n",
    "        for couple in lines:\n",
    "            tmp = couple.split(\"\\t\")\n",
    "            tmp[1] = tmp[1].rstrip('\\n')\n",
    "            couple_list.append(tmp)\n",
    "        \n",
    "        return couple_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_list_path = \"resource/term_list.txt\"\n",
    "\n",
    "couples = couple_list(term_list_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "for couple in couples:\n",
    "    w1 = couple[0]\n",
    "    w2 = couple[1]\n",
    "    nasari_sim = max_nasari_similarity(w1, w2)\n",
    "    # print(\"Similarity between \" + w1 + \" and \" + w2 + \" is \" + str(nasari_sim[0]))\n",
    "    # print(round(nasari_sim[0], 4))\n",
    "    # print(nasari_sim[1] + \"\\t\" + nasari_sim[2])\n",
    "    values.append(round(nasari_sim[0], 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Spearman e Pearson tra media similarità annotatori e cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = values\n",
    "# sim_mean = [1.9, 2.3, 0, 1.8, 1, 2.15, 2, 0, 0.9, 1.75, 1.9, 2, 1.95, 2.8, 2, 3.75, 1.1, 0.8, 0, 0.35, 0.75, 3, 0.4, 1.4, 2.6, 2.8, 3.5, 2.75, 1, 2.5, 0, 1.3, 4, 2.8, 0.2, 2.5, 2.25, 2.5, 2.4, 2.1, 0, 2.5, 2.15,1.5, 2.7, 3.3, 1.65, 0, 0, 1.8]\n",
    "sim_norm_mean = [0.475,0.575,0,0.45,0.25,0.5375,0.5,0,0.225,0.4375,0.475,0.5,0.4875,0.7,0.5,0.9375,0.275,0.2,0,0.0875,0.1875,0.75,0.1,0.35,0.65,0.7,0.875,0.6875,0.25,0.625,0,0.325,1,0.7,0.05,0.625,0.5625,0.625,0.6,0.525,0,0.625,0.5375,0.375,0.675,0.825,0.4125,0,0,0.45]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4396"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(pearson_index_scipy(cos_sim, sim_norm_mean), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61799"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(spearman_index_scipy(cos_sim, sim_norm_mean), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrazione dei sinonimi dei termini per comporre l'output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output costituito da 6 campi: \n",
    "   1. Term1\n",
    "   2. Term2\n",
    "   3. BS1 (dato da noi)\n",
    "   4. BS2 (dato da noi)\n",
    "   5. Sinonimi Term1 (da synset a lemma)\n",
    "   6. Sinonimi Term2 (da synset a lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KEY = '8be6bdbd-78ee-4efb-91a2-3854f79a97e0'\n",
    "KEY = '4ac4317e-cf96-4222-ba17-753d12dc7a2f'\n",
    "URL = 'https://babelnet.io/v7/getSynset?id={}&key={}&targetLang=IT&searchLang=IT'\n",
    "    \n",
    "def get_synonims_for_synsets(babel_synsets_ids):\n",
    "    '''\n",
    "    Given a list of babelNetSynset IDs returns a dictionary containing,\n",
    "    for each ID, the set of all lemmas in the corresponding BabelNet Synset.\n",
    "    '''\n",
    "    res = dict()\n",
    "    for id in babel_synsets_ids:\n",
    "        if id in res.keys(): \n",
    "            continue\n",
    "        res[id] = set()\n",
    "        x = requests.get(URL.format(id,KEY))\n",
    "        if x.status_code == 400:\n",
    "            res[id] = list(res[id])\n",
    "            continue\n",
    "        senses = x.json()['senses']\n",
    "        for sense in senses:\n",
    "            res[id].add(sense['properties']['fullLemma'])\n",
    "        res[id] = list(res[id])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_1 = []\n",
    "terms_2 = []\n",
    "for el in couples:\n",
    "    terms_1.append(el[0])\n",
    "    terms_2.append(el[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vado ad estrarre i synset dal file di testo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_list_path = \"resource/synset_lists.txt\"\n",
    "syns = []\n",
    "with open(syn_list_path) as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        syns.append(line.strip(\"\\n\").split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs1_p = syns[0]\n",
    "bs2_p = syns[1]\n",
    "\n",
    "bs1_d = syns[2]\n",
    "bs2_d = syns[3]\n",
    "\n",
    "nasari_bs1 = syns[4]\n",
    "nasari_bs2 = syns[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "synsets = set(bs1_p).union(set(bs2_p), set(bs1_d), set(bs2_d), set(nasari_bs1), set(nasari_bs2)) \n",
    "synsets_syninoms = get_synonims_for_synsets(synsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "paolo_list = []\n",
    "davide_list = []\n",
    "for i in range(50):\n",
    "    paolo_list.append([terms_1[i], terms_2[i], bs1_p[i], bs2_p[i], synsets_syninoms[bs1_p[i]], synsets_syninoms[bs2_p[i]]])\n",
    "    davide_list.append([terms_1[i], terms_2[i], bs1_d[i], bs2_d[i], synsets_syninoms[bs1_d[i]], synsets_syninoms[bs2_d[i]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['partito politico',\n",
       " 'associazione',\n",
       " 'bn:00060834n',\n",
       " 'bn:14146654n',\n",
       " [\"L'origine_dei_partiti\",\n",
       "  'appartenenza_politica',\n",
       "  'parte',\n",
       "  'movimento_politico',\n",
       "  'Storia_dei_partiti_politici',\n",
       "  'gruppi_politici',\n",
       "  'politica_di_partito',\n",
       "  'forza_politica',\n",
       "  'partito',\n",
       "  'partiti_politici',\n",
       "  'partiti',\n",
       "  'partito_politico'],\n",
       " ['associazione', 'club', 'lega', 'ordine']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paolo_list[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['partito politico',\n",
       " 'associazione',\n",
       " 'bn:00060834n',\n",
       " 'bn:00006539n',\n",
       " [\"L'origine_dei_partiti\",\n",
       "  'appartenenza_politica',\n",
       "  'parte',\n",
       "  'movimento_politico',\n",
       "  'Storia_dei_partiti_politici',\n",
       "  'gruppi_politici',\n",
       "  'politica_di_partito',\n",
       "  'forza_politica',\n",
       "  'partito',\n",
       "  'partiti_politici',\n",
       "  'partiti',\n",
       "  'partito_politico'],\n",
       " ['organizzazione_di_volontariato',\n",
       "  'categoria',\n",
       "  'associazione_(diritto)',\n",
       "  'associazioni_non_riconosciute',\n",
       "  'associazione_senza_personalità_giuridica',\n",
       "  'associazione_non_riconosciuta',\n",
       "  'associazioni_prive_di_personalità_giuridica',\n",
       "  'consorzio',\n",
       "  'associazione',\n",
       "  'associazione_di_volontariato',\n",
       "  'classe',\n",
       "  'consociazione',\n",
       "  'corporazione',\n",
       "  'ordine']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "davide_list[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output pretty print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stampa Paolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biotopo\tbiologia\tbn:03353031n\tbn:00010543n\n",
      "biotopi,biotopo\tscienze_della_vita,Biologhe,scienza_biologica\n",
      "\n",
      "magma\tvulcano\tbn:00052703n\tbn:00079748n\n",
      "magmatico,differenziazione_magmatica,magmi\tvulcano\n",
      "\n",
      "brainstorming\ttelescopio\tbn:00012707n\tbn:00069738n\n",
      "il_brainstorming,Brain_storming,brainstorming\ttelescopio,cannocchiale,Montatura_del_telescopio\n",
      "\n",
      "livello\tpunteggio\tbn:00025965n\tbn:00041241n\n",
      "misura,voto,piano\tpunteggio,voto,marchio\n",
      "\n",
      "centesimo\taffare\tbn:00017162n\tbn:00014152n\n",
      "centesimi,centesimo,centime\tofferta_impresa,affare,accordo\n",
      "\n",
      "partito politico\tassociazione\tbn:00060834n\tbn:14146654n\n",
      "L'origine_dei_partiti,appartenenza_politica,parte\tassociazione,club,lega\n",
      "\n",
      "tsunami\tmare\tbn:00078509n\tbn:00069946n\n",
      "Run-up,onda_anomala,Zunami\tmare_interno,mare,acque\n",
      "\n",
      "struzzo\tfrutteto\tbn:00059688n\tbn:00041967n\n",
      "struthio_camelus,struzzi,struzzo\tbroilo,piantagione,boschetto\n",
      "\n",
      "cannella\tcaramella\tbn:00019142n\tbn:00015227n\n",
      "cinnamomo,cannella\tdolci,caramella,caramelle\n",
      "\n",
      "scopa\tpolvere\tbn:00013352n\tbn:00029193n\n",
      "scopa,granata,scope\tpolvere\n",
      "\n",
      "galassia\tastronomo\tbn:00032476n\tbn:00006659n\n",
      "nebulosa_extragalattica,rotazione_galattica,nebulosa\turanologist,astronoma,astrologo\n",
      "\n",
      "succo\tfrappè\tbn:00048532n\tbn:00055009n\n",
      "succhi_di_frutta_e_vegetali,succo,succo_di_frutta\tfrappé,Milk-shake,frullato\n",
      "\n",
      "tapparella\ttenda\tbn:00068154n\tbn:00024534n\n",
      "tapparella,avvolgibile\ttendaggio,tenda_(arredamento),tendina\n",
      "\n",
      "criminale\tcolpevole\tbn:05040795n\tbn:00024341n\n",
      "criminale\tresponsabile,reo,autore\n",
      "\n",
      "cancro al pancreas\tchemioterapia\tbn:00060358n\tbn:00018141n\n",
      "carcinoma_pancreatico,tumore_del_pancreas,carcinoma_del_pancreas\tchemio,agenti_antitumorali,chemioterapie\n",
      "\n",
      "passato\tantecedente\tbn:00060928n\tbn:00004490n\n",
      "passato\tantefatto,premessa,precedente\n",
      "\n",
      "Nazioni Unite\tParlamento Europeo\tbn:00078931n\tbn:03855948n\n",
      "UN,u.n,United_Nations\tVI_Legislatura_del_Parlamento_europeo,Parlamento_europeo,Europarlamento\n",
      "\n",
      "canzone\tesecutore\tbn:00072794n\tbn:00032171n\n",
      "canzoni,canzone,voce_umana\tesecutore\n",
      "\n",
      "pistola\ttaccuino\tbn:00042808n\tbn:00058156n\n",
      "pistola,tiro_di_ferro,rivoltella\ttaccuino,notepad,notes\n",
      "\n",
      "acetilcolina\tiride\tbn:00000853n\tbn:00047467n\n",
      "C7H16NO2,C7NH16O2,acetilcolina\tiride,iride_(anatomia),Iris\n",
      "\n",
      "alfabeto\tpenna\tbn:00003054n\tbn:00061314n\n",
      "alfabeto,simboli_di_input\tpenna,penna_(scrittura),penna_e_inchiostro\n",
      "\n",
      "coro\tcantante\tbn:00018782n\tbn:14993684n\n",
      "coro\tcanzonettista,Cantante_rock,cantante_solista\n",
      "\n",
      "legno\tcoperta\tbn:00081492n\tbn:00011119n\n",
      "Lignei,legname,lignee\tcoperta,copriletto,coltre\n",
      "\n",
      "soldato\tpace\tbn:00013585n\tbn:00045291n\n",
      "Jundī,soldato_semplice,buck_privato\tsilenzio,pace,quiete\n",
      "\n",
      "inglese\tamericano\tbn:00030877n\tbn:00003343n\n",
      "persona_inglese,britannico,inglese\tPopolo_statunitense,americano,statunitensi\n",
      "\n",
      "lucertola\tcoccodrillo\tbn:00051655n\tbn:00023894n\n",
      "lucertolina,sauri,lucertole\tCrocodylinae,coccodrillo,coccodrilli\n",
      "\n",
      "denaro\tcontante\tbn:00055646n\tbn:00016449n\n",
      "soldi,danaro,denaro\tcontante,pagamento_immediato\n",
      "\n",
      "Polpo Paul\tpolpo\tbn:02158646n\tbn:00131452n\n",
      "Polpo_Paul,polpo_Paul\tPolpi,Octopus_vulgaris,polpo_comune\n",
      "\n",
      "calendario\tvacanza\tbn:00014706n\tbn:00079444n\n",
      "calendario\tvacanza\n",
      "\n",
      "base\tsostanza chimica\tbn:00002771\tbn:00018096n\n",
      "\tchimica,prodotto_chimico,chimico\n",
      "\n",
      "governo\tlucertola\tbn:00007299n\tbn:00051655n\n",
      "esecutivo,autorità,consiglio_dei_ministri\tlucertolina,sauri,lucertole\n",
      "\n",
      "JPY\triciclaggio di denaro\tbn:00081896n\tbn:00055649n\n",
      "¥,yen_giapponese,JPY\tautoriciclaggio,riciclatore_di_denaro,il_riciclaggio_di_denaro\n",
      "\n",
      "DeepMind\tGoogle DeepMind\t\tbn:16362897n\n",
      "\tDeepMind,Google_DeepMind\n",
      "\n",
      "scacchi\tscacco al re\tbn:00018197n\tbn:00017990n\n",
      "partita_di_scacchi,scacchista,scacchi\tscacco,controllo,Scacco\n",
      "\n",
      "sonetto\tbellezza\tbn:00072807n\tbn:00009439n\n",
      "sonetto,sonetti\testetica,bello,bellezza\n",
      "\n",
      "lavoratore\tufficio\tbn:09081960n\tbn:00014169n\n",
      "lavoratori,lavoratore\tstudio,uffizio,business_office\n",
      "\n",
      "valuta\tscambio\tbn:00055646n\tbn:00032130n\n",
      "soldi,danaro,denaro\tpalleggio,scambio\n",
      "\n",
      "poliestere\tcotone\tbn:00063411n\tbn:00023072n\n",
      "poliestere\tcotone_(fibra),Mercato_del_cotone,Cotone_(filo)\n",
      "\n",
      "tribunale\tgiustizia\tbn:16597336n\tbn:00048668n\n",
      "aula_di_tribunale,tribunale\tingiustizie,equità,giustizia\n",
      "\n",
      "coda\tretro\tbn:00034422n\tbn:00007728n\n",
      "file,coda,fila\trovescio,dietro,di_dietro\n",
      "\n",
      "incidente\tlibro\tbn:00023571n\tbn:00012060n\n",
      "scontro,disastro,sciagura\tlibro,grafico_editoriale,tomo\n",
      "\n",
      "groppo\tvento\tbn:00073705n\tbn:00002216n\n",
      "piovasco,bufera,burrasca\traffiche_di_vento,vento,venti\n",
      "\n",
      "piano\tarmonia\tbn:00035986n\tbn:16786228n\n",
      "pianoforte_classico,pianoforte,pianoforti\tsintonia,armonia\n",
      "\n",
      "sultano\tministero\tbn:17340012n\tbn:00055179n\n",
      "sultano_di_Agrabah,imperatore_(Aladino_e_la_lampada_meravigliosa),Sultano\tministero\n",
      "\n",
      "deserto\tduna\tbn:00026519n\tbn:00029124n\n",
      "deserto_freddo,semideserto,deserto\tdune_costiere,dune,duna_di_sabbia\n",
      "\n",
      "moltiplicazione\tdivisione\tbn:00056300n\tbn:00027919n\n",
      "times,moltiplicare,moltiplicazione\tdivisione_(matematica),∕,divisione_intera\n",
      "\n",
      "virus\tsangue\tbn:00080085n\tbn:00011346n\n",
      "virus,virioni,infezioni_virali\tsangue,ematico,flusso_sanguigno\n",
      "\n",
      "era glaciale\tstatuto\tbn:00040575n\tbn:14285178n\n",
      "Era_Glaciale,glaciazione,glaciale\tstatuti_medioevali,statuto,statuti_medievali\n",
      "\n",
      "spada\tambiente\tbn:00011081n\tbn:00031075n\n",
      "durlindana,durindana,Spada_(arma)\tvariabili_di_ambiente,ambiente,circondare\n",
      "\n",
      "scuola elementare\tpenna\tbn:00030308n\tbn:00061314n\n",
      "primina,scuola_a_tempo_pieno,scuola_primaria_in_Italia\tpenna,penna_(scrittura),penna_e_inchiostro\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for el in paolo_list:\n",
    "    print(el[0] + \"\\t\" + el[1] + \"\\t\" + el[2] + \"\\t\" + el[3])\n",
    "    print(','.join(el[4][:3]) + \"\\t\" + ','.join(el[5][:3]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stampa Davide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biotopo\tbiologia\tbn:03353031n\tbn:00010543n\n",
      "biotopi,biotopo\tscienze_della_vita,Biologhe,scienza_biologica\n",
      "\n",
      "magma\tvulcano\tbn:00052703n\tbn:00079748n\n",
      "magmatico,differenziazione_magmatica,magmi\tvulcano\n",
      "\n",
      "brainstorming\ttelescopio\tbn:00012707n\tbn:00069738n\n",
      "il_brainstorming,Brain_storming,brainstorming\ttelescopio,cannocchiale,Montatura_del_telescopio\n",
      "\n",
      "livello\tpunteggio\tbn:00025965n\tbn:00041241n\n",
      "misura,voto,piano\tpunteggio,voto,marchio\n",
      "\n",
      "centesimo\taffare\tbn:00017162n\tbn:00014152n\n",
      "centesimi,centesimo,centime\tofferta_impresa,affare,accordo\n",
      "\n",
      "partito politico\tassociazione\tbn:00060834n\tbn:00006539n\n",
      "L'origine_dei_partiti,appartenenza_politica,parte\torganizzazione_di_volontariato,categoria,associazione_(diritto)\n",
      "\n",
      "tsunami\tmare\tbn:00078509n\tbn:00069946n\n",
      "Run-up,onda_anomala,Zunami\tmare_interno,mare,acque\n",
      "\n",
      "struzzo\tfrutteto\tbn:00059688n\tbn:00041967n\n",
      "struthio_camelus,struzzi,struzzo\tbroilo,piantagione,boschetto\n",
      "\n",
      "cannella\tcaramella\tbn:00019142n\tbn:00021707n\n",
      "cinnamomo,cannella\tdolce,confezione,dolciumi\n",
      "\n",
      "scopa\tpolvere\tbn:00010088n\tbn:00029192n\n",
      "scopa,besom,ramazza\tPolveri,polvere,polverio\n",
      "\n",
      "galassia\tastronomo\tbn:00055023n\tbn:00006659n\n",
      "la_Via_Lattea,Via_lattea,Galassia_Via_Lattea\turanologist,astronoma,astrologo\n",
      "\n",
      "succo\tfrappè\tbn:00048532n\tbn:00055009n\n",
      "succhi_di_frutta_e_vegetali,succo,succo_di_frutta\tfrappé,Milk-shake,frullato\n",
      "\n",
      "tapparella\ttenda\tbn:01256008n\tbn:00024534n\n",
      "Serranda,Tapparella,serrande\ttendaggio,tenda_(arredamento),tendina\n",
      "\n",
      "criminale\tcolpevole\tbn:05040795n\tbn:00011099n\n",
      "criminale\tSenso_di_colpa,Colpevole,torto\n",
      "\n",
      "cancro al pancreas\tchemioterapia\tbn:00060358n\tbn:00018141n\n",
      "carcinoma_pancreatico,tumore_del_pancreas,carcinoma_del_pancreas\tchemio,agenti_antitumorali,chemioterapie\n",
      "\n",
      "passato\tantecedente\tbn:00060927n\tbn:00004490n\n",
      "passato,tempi_passati,altri_tempi\tantefatto,premessa,precedente\n",
      "\n",
      "Nazioni Unite\tParlamento Europeo\tbn:00078931n\tbn:03855948n\n",
      "UN,u.n,United_Nations\tVI_Legislatura_del_Parlamento_europeo,Parlamento_europeo,Europarlamento\n",
      "\n",
      "canzone\tesecutore\tbn:00072794n\tbn:00032191n\n",
      "canzoni,canzone,voce_umana\tesecutori,esecutore_testamentario,esecutore\n",
      "\n",
      "pistola\ttaccuino\tbn:00042808n\tbn:00058156n\n",
      "pistola,tiro_di_ferro,rivoltella\ttaccuino,notepad,notes\n",
      "\n",
      "acetilcolina\tiride\tbn:00000853n\tbn:00047467n\n",
      "C7H16NO2,C7NH16O2,acetilcolina\tiride,iride_(anatomia),Iris\n",
      "\n",
      "alfabeto\tpenna\tbn:00003054n\tbn:00061314n\n",
      "alfabeto,simboli_di_input\tpenna,penna_(scrittura),penna_e_inchiostro\n",
      "\n",
      "coro\tcantante\tbn:00018665n\tbn:00071731n\n",
      "coro,canto_corale,cori_polifonici\tcantante,cantatore\n",
      "\n",
      "legno\tcoperta\tbn:00081492n\tbn:00011119n\n",
      "Lignei,legname,lignee\tcoperta,copriletto,coltre\n",
      "\n",
      "soldato\tpace\tbn:00072698n\tbn:00061149n\n",
      "milite,soldato_semplice,soldati_semplici\tStudi_per_la_pace,pace\n",
      "\n",
      "inglese\tamericano\tbn:00030877n\tbn:00003343n\n",
      "persona_inglese,britannico,inglese\tPopolo_statunitense,americano,statunitensi\n",
      "\n",
      "lucertola\tcoccodrillo\tbn:00051655n\tbn:00023894n\n",
      "lucertolina,sauri,lucertole\tCrocodylinae,coccodrillo,coccodrilli\n",
      "\n",
      "denaro\tcontante\tbn:00055644n\tbn:00016448n\n",
      "denaro,pecuniarie,moneta\tliquidi,contanti,denaro_in_contante\n",
      "\n",
      "Polpo Paul\tpolpo\tbn:02158646n\tbn:02158646n\n",
      "Polpo_Paul,polpo_Paul\tPolpo_Paul,polpo_Paul\n",
      "\n",
      "calendario\tvacanza\tbn:00014705n\tbn:00060178n\n",
      "calendario,calendari\tperiodo_annuale_di_ferie,periodi_di_ferie,vacanze\n",
      "\n",
      "base\tsostanza chimica\tbn:00002771n\tbn:00018096n\n",
      "alcali,base_(chimica),alcaline\tchimica,prodotto_chimico,chimico\n",
      "\n",
      "governo\tlucertola\tbn:00032180n\tbn:00051655n\n",
      "esecutivo,ramo_esecutivo_del_governo,potere_esecutivo\tlucertolina,sauri,lucertole\n",
      "\n",
      "JPY\triciclaggio di denaro\tbn:00081896n\tbn:00055649n\n",
      "¥,yen_giapponese,JPY\tautoriciclaggio,riciclatore_di_denaro,il_riciclaggio_di_denaro\n",
      "\n",
      "DeepMind\tGoogle DeepMind\t\tbn:16362897n\n",
      "\tDeepMind,Google_DeepMind\n",
      "\n",
      "scacchi\tscacco al re\tbn:00018206n\tbn:00017990n\n",
      "gioco_degli_scacchi,set_di_scacchi,scacchi\tscacco,controllo,Scacco\n",
      "\n",
      "sonetto\tbellezza\tbn:00072807n\tbn:00009439n\n",
      "sonetto,sonetti\testetica,bello,bellezza\n",
      "\n",
      "lavoratore\tufficio\tbn:09081960n\tbn:00014169n\n",
      "lavoratori,lavoratore\tstudio,uffizio,business_office\n",
      "\n",
      "valuta\tscambio\tbn:00024507n\tbn:03963674n\n",
      "divisa_(economia),moneta_circolante,mezzo_di_scambio\tScambio_commerciale,scambio,Scambio\n",
      "\n",
      "poliestere\tcotone\tbn:00063411n\tbn:00023072n\n",
      "poliestere\tcotone_(fibra),Mercato_del_cotone,Cotone_(filo)\n",
      "\n",
      "tribunale\tgiustizia\tbn:00023308n\tbn:00048669n\n",
      "organo_giurisdizionale,collegio_giudicante,corte_di_giustizia\tingiustizia,amministrazione_della_giustizia,giustizia_di_Dio\n",
      "\n",
      "coda\tretro\tbn:00030722n\tbn:00007728n\n",
      "conclusione,fondo,chiusa\trovescio,dietro,di_dietro\n",
      "\n",
      "incidente\tlibro\tbn:00023571n\tbn:00012059n\n",
      "scontro,disastro,sciagura\tstoria_del_libro,prima_di_copertina,controguardia\n",
      "\n",
      "groppo\tvento\tbn:00073705n\tbn:00002216n\n",
      "piovasco,bufera,burrasca\traffiche_di_vento,vento,venti\n",
      "\n",
      "piano\tarmonia\tbn:00035986n\tbn:00043030n\n",
      "pianoforte_classico,pianoforte,pianoforti\tarmonia\n",
      "\n",
      "sultano\tministero\tbn:00041348n\tbn:00014446n\n",
      "sultanati,sultanato,sultani\tconsiglio_esecutivo,Consiglio_esecutivo,consiglio_dei_ministri\n",
      "\n",
      "deserto\tduna\tbn:00026519n\tbn:00029124n\n",
      "deserto_freddo,semideserto,deserto\tdune_costiere,dune,duna_di_sabbia\n",
      "\n",
      "moltiplicazione\tdivisione\tbn:00056300n\tbn:00027919n\n",
      "times,moltiplicare,moltiplicazione\tdivisione_(matematica),∕,divisione_intera\n",
      "\n",
      "virus\tsangue\tbn:00080085n\tbn:00011392n\n",
      "virus,virioni,infezioni_virali\tcircolo_ematico,sangue,flusso_sanguigno\n",
      "\n",
      "era glaciale\tstatuto\tbn:00040575n\tbn:00017906n\n",
      "Era_Glaciale,glaciazione,glaciale\tcarta,statuto\n",
      "\n",
      "spada\tambiente\tbn:00011081n\tbn:03362870n\n",
      "durlindana,durindana,Spada_(arma)\tambiente_naturale,Ambienti_naturali,ambiente\n",
      "\n",
      "scuola elementare\tpenna\tbn:00030305n\tbn:00061314n\n",
      "istruzione_elementare,istruzione_primaria\tpenna,penna_(scrittura),penna_e_inchiostro\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for el in davide_list:\n",
    "    print(el[0] + \"\\t\" + el[1] + \"\\t\" + el[2] + \"\\t\" + el[3])\n",
    "    print(','.join(el[4][:3]) + \"\\t\" + ','.join(el[5][:3]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kappa di Cohen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inter-rater agreement tra stringhe, calcolata sui BabelNet Synset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Synset 1 a confronto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6756"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(cohen_kappa_score(bs1_p, bs1_d), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Synset 2 a confronto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63504"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(cohen_kappa_score(bs2_p, bs2_d), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valutazione annotazione: Nostra vs NASARI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contiamo quante volte abbiamo annotato un sysnet diverso rispetto a quello trovato massimizzando la cosine similarity nel file mini_NASARI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_annotation(syns_list1, syns_list_2):\n",
    "    ''' Return a list with the position of the different annotations '''\n",
    "    diff = []\n",
    "    for i in range(len(syns_list1)):\n",
    "        if(syns_list1[i] != syns_list_2[i]):\n",
    "            diff.append(i)\n",
    "    return diff\n",
    "\n",
    "def eval_annotation_couple(syns_list1, syns_list_2, syns_list_1a, syns_list_2a):\n",
    "    ''' Return a list with the position of the different annotations '''\n",
    "    diff = []\n",
    "    for i in range(len(syns_list1)):\n",
    "        if(syns_list1[i] != syns_list_1a[i] or syns_list_2[i] != syns_list_2a[i]):\n",
    "            diff.append(i)\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Syns_1 Paolo vs Nasari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_annotation(bs1_p, nasari_bs1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Syns_1 Davide vs Nasari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_annotation(bs1_d, nasari_bs1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Syns_2 Paolo vs Nasari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_annotation(bs2_p, nasari_bs2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Syns_2 Davide vs Nasari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_annotation(bs2_d, nasari_bs2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation coppie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Paolo vs NASARI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_annotation_couple(bs1_p, bs2_p, nasari_bs1, nasari_bs2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Davide vs NASARI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_annotation_couple(bs1_d, bs2_d, nasari_bs1, nasari_bs2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venvradicioni': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe30bb084dadcc2974aabac99d8c6a813637b467837e0e94701709de5a097010"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
