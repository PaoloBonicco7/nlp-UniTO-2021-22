{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizio 2 - Content2Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo esercizio cerchiamo di risalire al termine partendo dalle definizioni date a lezioni e che abbiamo già utilizzato nell'esercizio 1.\n",
    "Visionare \"es1_defs\" per ulteriori approfondimenti sul dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approccio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prendiamo i terminini più frequenti nelle definizioni, che saranno i genus.\n",
    "  *Perchè i Genus?* Così possiamo restringere la ricerca in wordnet, confrontando le nostre definizioni con un sottoinsieme di definizioni\n",
    "  al posto di tutte le definizioni di wordnet, passando da migliaia di definizioni ad un centinaio, in modo da rendere la ricerca più efficiente.\n",
    "  \n",
    "- *Text cleaning*: Stopwords removing e lemmatization\n",
    "- Preleviamo tutto il sottoalbero di iponimi dei genus\n",
    "- Prendiamo le definizioni (glossa) dei synset, ricavati dagli iponimi trovati al passo precedente\n",
    "- Facciamo il confronto tra definizioni di wordnet e la nostra lista di definizioni\n",
    "  *Come confontiamo le definizioni?* Andiamo a scegliere come termine quello che nella definizione ha più parole in comune con le nostre definizioni.\n",
    "- Restituisco il synset che ha òa definizione più simile a quella della lista"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from gensim.test.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_file(path):\n",
    "    '''\n",
    "    Read a file and, after revoving all stopwords, return a list of words.\n",
    "    '''\n",
    "    file = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    with open (path, 'r') as f:\n",
    "        for row in f:\n",
    "            filtered_s = [w for w in word_tokenize(row) if not w.lower() in stop_words]\n",
    "            file.append(simple_preprocess(str(filtered_s), deacc=True))\n",
    "    f.close()\n",
    "    return file\n",
    "\n",
    "def get_most_freq_words(text, nword):\n",
    "    '''\n",
    "    Given a list of sententeces, return the nword most frequent words\n",
    "    in each row of the document.\n",
    "    '''\n",
    "    genus = []\n",
    "    for row in text:\n",
    "        c = Counter()\n",
    "        c.update(row)\n",
    "        genus.append(c.most_common(nword))\n",
    "    return genus\n",
    "\n",
    "def get_hypos(word):\n",
    "    '''\n",
    "    Return all the hyponyms of a word.\n",
    "    '''\n",
    "    syn = get_synset(word)\n",
    "    hypo_list = []\n",
    "    if(syn is not None):\n",
    "        hypo_list = list(set([w for s in syn.closure(lambda s:s.hyponyms()) for w in s.lemma_names()]))\n",
    "    return hypo_list\n",
    "\n",
    "def get_hypers(word):\n",
    "    '''\n",
    "    Return the direct hypernyms of a word.\n",
    "    '''\n",
    "    syn = get_synset(word)\n",
    "    hypo_list = []\n",
    "    if(syn is not None):\n",
    "        hypo_list = list(set([w for s in syn.closure(lambda s:s.hypermyms()) for w in s.lemma_names()]))\n",
    "    return hypo_list\n",
    "\n",
    "def get_synset(word):\n",
    "    '''\n",
    "    Retrurn the first synset of a word.\n",
    "    '''\n",
    "    if(len(wn.synsets(word)) > 0):\n",
    "        return wn.synsets(word)[0]\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo a modificare questi due valori si ottengono risultati diversi. \n",
    "Dopo alcuni test abbiamo scelto questi valori siccome erano quelli che retituivano risultati migliori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_genus = 3 # number of most frequent words to search in the definitions file, used to determine genus\n",
    "num_most_freq_word = 10 # number of most frequent words to search in the definitions, used to compare with the wordnet's definitions\n",
    "\n",
    "starting_words = [\"Emotion\", \"Person\", \"Revenge\", \"Brick\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing data and find the genus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('feeling', 11), ('human', 8), ('feel', 8)], [('human', 26), ('person', 5), ('homo', 5)], [('someone', 14), ('feeling', 7), ('anger', 7)], [('used', 22), ('object', 15), ('material', 13)]]\n"
     ]
    }
   ],
   "source": [
    "file = get_text_from_file('../data/def.csv')\n",
    "\n",
    "genus = get_most_freq_words(file, num_genus)\n",
    "\n",
    "print (genus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizziamo 3 genus in modo da aumentare l'accuratezza:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['feeling', 'human', 'feel'],\n",
       " ['human', 'person', 'homo'],\n",
       " ['someone', 'feeling', 'anger'],\n",
       " ['used', 'object', 'material']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genus_list = []\n",
    "for el in genus:\n",
    "    genus_list_inner = []\n",
    "    for el2 in el:\n",
    "        genus_list_inner.append(el2[0])\n",
    "    genus_list.append(genus_list_inner)\n",
    "        \n",
    "genus_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Word: Emotion /  Genus --> ['feeling', 'human', 'feel']\n",
      "Word: *appetite*, score: *3*\n",
      "Word: *boredom*, score: *3*\n",
      "Word: *wonderment*, score: *3*\n",
      "Word: *disapproval*, score: *3*\n",
      "Word: *relief*, score: *3*\n",
      "\n",
      "Original Word: Person /  Genus --> ['human', 'person', 'homo']\n",
      "Word: *Paleo-American*, score: *2*\n",
      "Word: *Khanty*, score: *2*\n",
      "Word: *Cochimi*, score: *2*\n",
      "Word: *Kui*, score: *2*\n",
      "Word: *Basque*, score: *2*\n",
      "\n",
      "Original Word: Revenge /  Genus --> ['someone', 'feeling', 'anger']\n",
      "Word: *sounding_board*, score: *5*\n",
      "Word: *stolidity*, score: *4*\n",
      "Word: *hate*, score: *4*\n",
      "Word: *unemotionality*, score: *4*\n",
      "Word: *impassivity*, score: *4*\n",
      "\n",
      "Original Word: Brick /  Genus --> ['used', 'object', 'material']\n",
      "Word: *brick*, score: *5*\n",
      "Word: *building_material*, score: *4*\n",
      "Word: *kaolin*, score: *3*\n",
      "Word: *writing_paper*, score: *3*\n",
      "Word: *manure*, score: *3*\n"
     ]
    }
   ],
   "source": [
    "# Extract the most used word in the definitions\n",
    "key_words_defs = get_most_freq_words(file, num_most_freq_word)\n",
    "\n",
    "for i in range(len(genus_list)):\n",
    "    \n",
    "    # Top 10 word used in the definitions\n",
    "    key_row = []\n",
    "    for el in key_words_defs[i]:\n",
    "        key_row.append(el[0])\n",
    "    \n",
    "    #* Version with 1 genun\n",
    "    # Get the hyponyms of the genus and find the definition of the hyponyms\n",
    "    # hypo_list = get_hypos(genus_list[i])\n",
    "    # print(hypo_list)\n",
    "    # hypo_def = []\n",
    "    # for hypo in hypo_list:\n",
    "    #     hypo_def.append((hypo, get_synset(hypo).definition()))\n",
    "    \n",
    "    #* version with multiple genus\n",
    "    hypo_list, hypo_def = [], []\n",
    "    for el in genus_list[i]:\n",
    "        hypo_list.append(get_hypos(el))\n",
    "        \n",
    "    hypo_list = [x for xs in hypo_list for x in xs]\n",
    "    for hypo in hypo_list:\n",
    "        hypo_def.append((hypo, get_synset(hypo).definition()))\n",
    "    \n",
    "    # Compare the definition of our definitions (def.csv file) with the definition of the hyponyms\n",
    "    res = []\n",
    "    for wndef in hypo_def: # Definition of the hyponyms in wordnet\n",
    "        score = 0\n",
    "        imp_words = []\n",
    "        for key_word in key_row: # Definition given by us\n",
    "            if(key_word in wndef[1]):\n",
    "                score += 1\n",
    "                imp_words.append(key_word)      \n",
    "        \n",
    "        # Store all the value\n",
    "        res.append((score, wndef[0], imp_words, wndef[1]))\n",
    "        \n",
    "    sorted_list = sorted(res, key=lambda x: x[0])\n",
    "    sorted_res = list(reversed(sorted_list))\n",
    "    print(\"\\nOriginal Word:\", starting_words[i], \"/  Genus -->\",genus_list[i])\n",
    "    for k in range(min(len(sorted_res), 5)):\n",
    "        #* Long print\n",
    "        # print(f'Word: *{sorted_res[k][1]}*, score: *{sorted_res[k][0]}*, the key words are *{sorted_res[k][2]}* and the definition is *{sorted_res[k][3]}*')\n",
    "        print(f'Word: *{sorted_res[k][1]}*, score: *{sorted_res[k][0]}*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisi dei risultati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nella stampa sopra abbiamo i genus trovati e i relativi iponimi con grado di similarità maggiore.\n",
    "\n",
    "Come possiamo vedere l'algoritmo non lavora bene, infatti riesce solo a trovare *brick* e lo trova in prima posizione (bene!). \n",
    "Per i restanti non ci va nemmeno vicino, infatti non compaiono i termini di riferimento nei primi 5 risultati, cerchiamo di capire come mai:\n",
    "\n",
    "1. **I genus**: Come possiamo vedere i genus di riferimento hanno poco a che vedere con i termini, come ad esempio *\"someone\"* per *revenge*.\n",
    "   Questo è portato dal dataset in input. Si potrebbe ripulire il dataset rimuovendo le parole che non hanno a che vedere con il termine originale,\n",
    "   ma così facendo si andrebbe a compromettere l'esercizio e si renderebbe questo approccio poco scalabile su altre basi di dati.\n",
    "\n",
    "2. **Iponimi**: Non è detto che il termine che stiamo cercando sia un suo iponimo dei genus che abbiamo selezioanto, infatti potrebbe essere \n",
    "   un iperonimo o essere proprio in un punto completamente diverso dell'albero di wordnet. Un possibile miglioramento dell'algoritmo potrebbe\n",
    "   essere quello di andare a prelevare altri synset oltre agli iponimi del genus, ad esempio andando a prelevare anche i fratelli del genus,\n",
    "   senza allontarsi troppo per non far esplodere la complessità dell'algoritmo.\n",
    "\n",
    "3. **Funzione di similarità**: Potremmo decidere anche di utilizzare altre funzioni di similarità oltre a quella basata sulla comparazione dei termini \n",
    "   più frequenti. L'algoritmo è basato su uno *score* che corrisponde a quante parole simili ci sono nelle definizioni. Potremmo andare ad \n",
    "   aumentare lo *score* sulla base di altri fattori, come la funzione di similarità di wordnet *path_similarity*.\n",
    "\n",
    "Infine, anche se i risultati non sono ottimi, non ci stupiamo del fatto che siamo riusciti a trovare *brick* siccome, almeno in teoria, è il\n",
    "termine più facile a cui dare una definizione."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
