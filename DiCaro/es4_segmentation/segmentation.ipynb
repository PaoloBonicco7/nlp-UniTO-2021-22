{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizio 4 - Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approccio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algoritmo è *brute force* perchè per trovare i tagli corretti li prova tutti generandoli in maniera randomica e associato ad ogni\n",
    "sequenza di tagli un punteggio. Il punteggio è calcolato come la somma degli n termini più frequnti all'interno di ogni segmento.\n",
    "\n",
    "L'idea alla base è che per ogni argomento ci siano un numero di parle che occorrono molte volte, e quindi l'algoritmo andrà a cercare \n",
    "i tagli che vanno a massimizzare questi valori, e che quindi vanno ad isolare i termini più frequenti per ogni segmento.\n",
    "\n",
    "L'algoritmo ha bisogno del numero di tagli in ingresso.\n",
    "\n",
    "- **COOCCORENCE** : In questo notebook corrispnde alla frequenza delle parole più utilizzate nel segmento\n",
    "\n",
    "Il dataset è stato ripulito rimuovendo tutte le stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*data/segmentation_eng*: Il dataset è costituito da pezzi dei paragrafi di wikipedia di 4 argomenti diversi, in questo caso i 3 argomenti sono:\n",
    "  - Gorillas\n",
    "  - Quantum Computing\n",
    "  - Astronomy\n",
    "  - Kendrick Lamar Biography\n",
    "\n",
    "I tagli corretti sono alla riga 59/60, alla riga 102/103 e alla riga 181/182.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from gensim.test.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words_ita(phrase):\n",
    "    stop_words = stopwords.words('italian')\n",
    "    phrase = phrase.split()\n",
    "    phrase = [word for word in phrase if word not in stop_words]\n",
    "    return phrase\n",
    "\n",
    "def get_text_from_file(path):\n",
    "    file = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    with open (path, 'r') as f:\n",
    "        for row in f:\n",
    "            filtered_s = [w for w in word_tokenize(row) if not w.lower() in stop_words]\n",
    "            file.append(simple_preprocess(str(filtered_s), deacc=True))\n",
    "    f.close()\n",
    "    return file\n",
    "\n",
    "def cooccorrence(text, n_most_words):\n",
    "    '''\n",
    "    Calculates the cooccorrence value of a sequence of words. \n",
    "    This value correspond to the sum of the occurrences of the n most frequently used words in the word list.\n",
    "    '''\n",
    "    score = 0\n",
    "    c = Counter()\n",
    "    most_common = []\n",
    "    for row in text:\n",
    "        c.update(row)\n",
    "        \n",
    "    most_common = c.most_common(n_most_words)\n",
    "    # print(most_common)\n",
    "    \n",
    "    for el in most_common:\n",
    "        score = score + el[1]\n",
    "        \n",
    "    return score\n",
    "\n",
    "def extract_segment(file, start, end):\n",
    "    '''\n",
    "    Given the first and the last line, extract the segment.\n",
    "    '''\n",
    "    segment = []\n",
    "    for i in range(start, end):\n",
    "        segment.append(file[i])\n",
    "    return segment\n",
    "\n",
    "def get_cuts(ntopic):\n",
    "    '''\n",
    "    Generate a list of random value between 1 and the number of lines in the documents,\n",
    "    that correspond to the cuts of the documents. The first value is 0 and the last is\n",
    "    the numebers of lines in the documents.\n",
    "    '''\n",
    "    cut = [0] * ntopic\n",
    "    while not(all(cut[i] < cut[i+1] for i in range(len(cut) - 1))):\n",
    "        for k in range(1, ntopic): # generate the cuts\n",
    "            cut[k] = random.randint(1, num_lines-1)\n",
    "    cut.append(num_lines)\n",
    "        \n",
    "    return cut\n",
    "\n",
    "def load_file(file_path):\n",
    "    file = get_text_from_file(file_path)\n",
    "    c = Counter()\n",
    "    num_lines = sum(1 for line in open(file_path)) # Number of lines in the file\n",
    "\n",
    "    for row in file:\n",
    "        c.update(row)\n",
    "        \n",
    "    return num_lines, file, c\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_scores(file, ntopic, n_most_words, n_iteration): # n_most_words = number of most used words, used in cooccorrence()\n",
    "    scores = [0]*ntopic\n",
    "    sum_scores, max = 0, 0\n",
    "    max_cut = []\n",
    "    \n",
    "    for f in range(n_iteration): \n",
    "        # First generate the random cuts\n",
    "        cut = get_cuts(ntopic)\n",
    "        \n",
    "        # Extract the segments by the cuts and calculate the cooccorrence value for each segment\n",
    "        # based on the random cuts\n",
    "        for i in range(len(cut)-1):\n",
    "            text = extract_segment(file, cut[i], cut[i+1])\n",
    "            scores[i] = cooccorrence(text, n_most_words)\n",
    "        \n",
    "        # Evaluate the result and store the best result\n",
    "        sum_scores = sum(scores)\n",
    "        if(sum_scores > max):\n",
    "            max = sum_scores\n",
    "            max_cut = cut\n",
    "            \n",
    "    return max_cut, max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('lamar', 78), ('quantum', 63), ('gorilla', 34), ('gorillas', 31), ('released', 30), ('also', 27), ('astronomy', 25), ('album', 25), ('first', 24), ('dre', 19), ('classical', 18), ('mixtape', 17), ('algorithm', 16), ('stars', 16), ('silverback', 15), ('early', 15), ('astronomical', 15), ('ray', 15), ('song', 15), ('years', 14), ('females', 14), ('made', 14), ('new', 14), ('computers', 14), ('computer', 14), ('wavelengths', 14), ('video', 14), ('males', 13), ('known', 13), ('may', 13)]\n"
     ]
    }
   ],
   "source": [
    "n_most_words = 3\n",
    "n_topics = 4\n",
    "n_iteration = 20000 # Number of iteration to generate the best result - higher value = more time and better result\n",
    "c = Counter()\n",
    "file = '../data/segmentation_eng.txt'\n",
    "\n",
    "file_data = load_file(file) # Load the file and get the number of lines and the list of most common word in the file\n",
    "\n",
    "num_lines = file_data[0]\n",
    "file = file_data[1]\n",
    "\n",
    "for row in file:\n",
    "    c.update(row)\n",
    "    \n",
    "print(c.most_common(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best cut at lines -->  [0, 59, 105, 181, 254]\n",
      "the max sum is    -->  363\n"
     ]
    }
   ],
   "source": [
    "res = max_scores(file, n_topics, n_most_words, n_iteration)\n",
    "\n",
    "print(f'\\nBest cut at lines -->  {res[0]}\\nthe max sum is    -->  {res[1]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisi dei risultati e limiti di questo approccio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per quanto semplice come approccio riesce a trovare i tegli corretti in poche esecuzioni sul nostro dataset, ma presenta diversi problemi:\n",
    "\n",
    "- **scala male su testi grossi**: Purtroppo l'algoritmo per come è stato studiato scala male su dataset più grossi perchè deve esaminare\n",
    "  più righe di testo e deve provare più combinazioni di taglie essendoci più righe ed eventualmente più tagli.\n",
    "\n",
    "- **similarity**: Per valutare la similarity all'interno di un documento prendiamo la somma dei termini più frequenti all'interno di ogni segmento.\n",
    "  Un possibile miglioramento è quello di trasformare il testo in un vettore e calcolare la cosine similarity del segmento e si va ad effettuare \n",
    "  il taglio dove la similarity va a decrescere per poi risalire, come abbiamo visto a lezione. Abbiamo deciso di non seguire questa strada per \n",
    "  trovare un approccio alternativo, anche se abbiamo ottenuto permormance peggiori sia in terimini di tempi di esecuzione che di precisione dei\n",
    "  tagli.\n",
    "\n",
    "- **dataset**: L'algoritmo performa bene su questo testo perchè in ogni segmento è presente una grande quantità di termini \"unici\", o meglio \n",
    "  che compaiono solamente all'interno di quel segmento. Se si tenta di applicare questo algoritmo a un dataset meno eterogeneo è probabile\n",
    "  che i tagli siano più imprecisi.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venvdicaro': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3fc711e4ee2b6550a1e55fb58a23702f691a325dedbbea2e10fa55c056de4b98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
