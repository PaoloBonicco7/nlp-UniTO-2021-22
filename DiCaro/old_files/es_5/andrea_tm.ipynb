{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim.test.utils import simple_preprocess\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_file(path):\n",
    "    file = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    with open (path, 'r') as f:\n",
    "        for row in f:\n",
    "            filtered_s = [w for w in word_tokenize(row) if not w.lower() in stop_words]\n",
    "            file.append(simple_preprocess(str(filtered_s), deacc=True))\n",
    "    f.close()\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    stoplist = set('for a of the and to in'.split(' '))\n",
    "    texts = [[word for word in document.lower().split() if word not in stoplist] for document in corpus]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequency(texts):  \n",
    "    frequency = defaultdict(int)\n",
    "    for text in texts:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.463*\"first\" + 0.220*\"messi\" + 0.203*\"barcelona\" + 0.202*\"goal\" + '\n",
      "  '0.202*\"team\" + 0.193*\"played\" + 0.175*\"goals\" + 0.159*\"may\" + '\n",
      "  '0.153*\"scored\" + 0.138*\"season\"'),\n",
      " (1,\n",
      "  '-0.383*\"th\" + -0.274*\"subspecies\" + -0.233*\"vary\" + -0.233*\"populations\" + '\n",
      "  '-0.221*\"described\" + -0.214*\"within\" + -0.191*\"length\" + -0.191*\"size\" + '\n",
      "  '-0.191*\"fur\" + -0.191*\"patterns\"'),\n",
      " (2,\n",
      "  '0.455*\"throw\" + 0.455*\"free\" + 0.395*\"percentage\" + 0.270*\"season\" + '\n",
      "  '0.184*\"career\" + 0.156*\"shooter\" + 0.156*\"high\" + -0.144*\"first\" + '\n",
      "  '0.144*\"end\" + 0.137*\"curry\"')]\n"
     ]
    }
   ],
   "source": [
    "text = get_text_from_file('../topics.txt')\n",
    "common_dictionary = Dictionary(text)\n",
    "common_corpus = [common_dictionary.doc2bow(line) for line in text]\n",
    "\n",
    "lsi_model = models.LsiModel(common_corpus, id2word = common_dictionary, num_topics=3)\n",
    "pprint(lsi_model.print_topics(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 2), (2, 2), (3, 1)],\n",
      " [(0, 1), (3, 1), (4, 1), (5, 2), (6, 1)],\n",
      " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1)]]\n"
     ]
    }
   ],
   "source": [
    "texts = tokenize(text_corpus)\n",
    "frequency = get_frequency(texts)\n",
    "processed_corpus = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "\n",
    "pprint(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '-0.707*\"learning\" + -0.500*\"data.\" + -0.500*\"machine\" + 0.000*\"is\" + -0.000*\"natural\" + -0.000*\"language\" + -0.000*\"process\"')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(corpus) \n",
    "\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "'''for doc in corpus_tfidf:\n",
    "    print(doc)'''\n",
    "\n",
    "lsi_model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=1)\n",
    "corpus_lsi = lsi_model[corpus_tfidf]\n",
    "\n",
    "lsi_model.print_topics(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6847c98a8f86b01c6a19c518cd2f366693b80566b266804d5ca763cbb223f52b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
