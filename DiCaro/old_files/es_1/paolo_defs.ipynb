{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizio 1\n",
    "In questo esercizio calcoliamo i valori di similarità tra le definizioni date da noi per 4 termini.\n",
    "Lo scopo dell'esercizio è provare che dare una definizione di un termine è più complicato di quanto si possa pensare, \n",
    "specialmente se il termine è di natura astratta e/o generico.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of stopword to be removed from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = ['on','of','a', 'most', 'be', 'did', \"mightn't\", 'or', 'the', 'does', \"it's\", 'ourselves', 'if', 'few', 'above', 've', 'ours', \n",
    "'your', 'some', 'own', 't', 'yours', 'couldn', \"you'll\", 'both', \"wouldn't\", 'once', 'off', 'doesn', 'through', 'their', \n",
    "'themselves', 'until', 'isn', 'do', \"hadn't\", 'have', 'from', 'needn', 'hers', 'has', 'between', 'not', 'ain', 'they', 'after', 'out', \n",
    "'then', 'while', \"shouldn't\", 'mightn', 'against', \"should've\", \"couldn't\", 'she', 'but', 'as', 'below', 'over', 'each', 'hadn', 'when', \n",
    "'of', 'there', 'hasn', 'before', 'aren', 'only', 'them', 'is', 'will', 'yourself', 'so', 're', 'very', \"you'd\", 'all', 'nor', 'o', 'haven', \n",
    "'had', 'that', 'doing', 'just', 'no', 'and', \"needn't\", 'was', \"didn't\", 'a', 'weren', 'why', 'an', \"mustn't\", \"isn't\", 'wouldn', 'whom', \n",
    "'too', \"that'll\", 'should', 'himself', 'about', 'which', 'under', \"don't\", 'y', \"hasn't\", 'been', 'his', 'here', 'further', \"doesn't\", \n",
    "'same', 'how', 'we', 'than', 'ma', 'who', 'herself', 'theirs', 'were', 'any', 'wasn', \"haven't\", 'having', 'it', 'yourselves', 'more', \n",
    "'won', 'those', 'by', 'now', 'd', 'where', 'me', 'him', 'again', 's', 'are', 'shouldn', \"weren't\", \"wasn't\", 'for', 'what', \"she's\", 'll', \n",
    "\"won't\", 'this', 'because', 'm', 'you', 'shan', 'up', 'can', 'her', 'itself', 'i', \"you're\", 'in', 'myself', 'its', 'mustn', 'don', 'these', \n",
    "'such', 'down', 'our', 'into', \"shan't\", 'didn', 'am', 'he', 'to', 'my', \"aren't\", 'other', \"you've\", 'at', 'during', 'with', 'someone']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "STOP_WORDS = ['on','of','a', 'most', 'be', 'did', \"mightn't\", 'or', 'the', 'does', \"it's\", 'ourselves', 'if', 'few', 'above', 've', 'ours', \n",
    "'your', 'some', 'own', 't', 'yours', 'couldn', \"you'll\", 'both', \"wouldn't\", 'once', 'off', 'doesn', 'through', 'their', \n",
    "'themselves', 'until', 'isn', 'do', \"hadn't\", 'have', 'from', 'needn', 'hers', 'has', 'between', 'not', 'ain', 'they', 'after', 'out', \n",
    "'then', 'while', \"shouldn't\", 'mightn', 'against', \"should've\", \"couldn't\", 'she', 'but', 'as', 'below', 'over', 'each', 'hadn', 'when', \n",
    "'of', 'there', 'hasn', 'before', 'aren', 'only', 'them', 'is', 'will', 'yourself', 'so', 're', 'very', \"you'd\", 'all', 'nor', 'o', 'haven', \n",
    "'had', 'that', 'doing', 'just', 'no', 'and', \"needn't\", 'was', \"didn't\", 'a', 'weren', 'why', 'an', \"mustn't\", \"isn't\", 'wouldn', 'whom', \n",
    "'too', \"that'll\", 'should', 'himself', 'about', 'which', 'under', \"don't\", 'y', \"hasn't\", 'been', 'his', 'here', 'further', \"doesn't\", \n",
    "'same', 'how', 'we', 'than', 'ma', 'who', 'herself', 'theirs', 'were', 'any', 'wasn', \"haven't\", 'having', 'it', 'yourselves', 'more', \n",
    "'won', 'those', 'by', 'now', 'd', 'where', 'me', 'him', 'again', 's', 'are', 'shouldn', \"weren't\", \"wasn't\", 'for', 'what', \"she's\", 'll', \n",
    "\"won't\", 'this', 'because', 'm', 'you', 'shan', 'up', 'can', 'her', 'itself', 'i', \"you're\", 'in', 'myself', 'its', 'mustn', 'don', 'these', \n",
    "'such', 'down', 'our', 'into', \"shan't\", 'didn', 'am', 'he', 'to', 'my', \"aren't\", 'other', \"you've\", 'at', 'during', 'with', 'someone']\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    result = []\n",
    "    for word in text:\n",
    "        if word.lower() not in STOP_WORDS:\n",
    "            result.append(word.lower())\n",
    "    return result\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    result = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for word in text:\n",
    "        result.append(lemmatizer.lemmatize(word))\n",
    "    return result\n",
    "\n",
    "list = open(\"def.csv\").readlines()\n",
    "data = []\n",
    "for l in list:\n",
    "    data.append(l.strip(\"\\n\").split(\",\"))\n",
    "\n",
    "for l in data:\n",
    "    main_concept = l[0]\n",
    "    counter_concept = Counter()\n",
    "    for definition in l[1:]:\n",
    "        statement = remove_stop_words(definition.split())\n",
    "        statement = lemmatize_words(statement)\n",
    "        counter_concept.update(statement)\n",
    "\n",
    "    first,second,third = counter_concept.most_common(3)\n",
    "\n",
    "    #print(f\"Parole più frequenti per {main_concept}: {counter_concept.most_common(3)}\")\n",
    "\n",
    "    results = {first[0]:0, second[0]:0, third[0]:0}\n",
    "    n_statement = 0\n",
    "\n",
    "    for definition in l[1:]:\n",
    "        if first[0] in definition.split():\n",
    "            results[first[0]] += 1\n",
    "        if second[0] in definition.split():\n",
    "            results[second[0]] += 1\n",
    "        if third[0] in definition.split():\n",
    "            results[third[0]] += 1\n",
    "        n_statement += 1\n",
    "    \n",
    "  \n",
    "    #* EVALUATION\n",
    "    mean = 0.0\n",
    "    for element in results.keys():\n",
    "        mean += results[element]/n_statement\n",
    "    print(f\"Punteggio finale per {main_concept}: {round(mean/3, 2)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Extract the data from csv\n",
    "df = pd.DataFrame(open('../def.csv'))\n",
    "\n",
    "# Create stemmer - porter works better than english\n",
    "stemmer2 = SnowballStemmer(\"porter\")\n",
    "\n",
    "# Save the dataframe in different variables and remove the useless columns\n",
    "emotion_defs = df.loc[0,\"P1\":].dropna().replace(',','', regex=True)\n",
    "# person_defs = df.loc[1,\"P1\":].dropna().replace(',','', regex=True)\n",
    "# revenge_defs = df.loc[2,\"P1\":].dropna().replace(',','', regex=True)\n",
    "# brick_def = df.loc[3,\"P1\":].dropna().replace(',','', regex=True)\n",
    "\n",
    "# lists_of_defs = [emotion_defs, person_defs, revenge_defs, brick_def]\n",
    "\n",
    "# Create a list of all the defs after stop word removing and stemming\n",
    "list_defs = []\n",
    "for definition in emotion_defs:\n",
    "    text_tokens = word_tokenize(definition) # ['Range', 'of', 'concepts', 'human', 'beings', 'feel', 'in', 'certain', 'situations']\n",
    "    \n",
    "    # remove stop words\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "    \n",
    "    tokens_without_sw_stm = []\n",
    "    for word in tokens_without_sw:\n",
    "        word = stemmer2.stem(word)\n",
    "        tokens_without_sw_stm.append(word)\n",
    "    list_defs.append(tokens_without_sw_stm)    \n",
    "    \n",
    "print(list_defs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity with the word2vec method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(word):\n",
    "    from collections import Counter\n",
    "    from math import sqrt\n",
    "\n",
    "    # count the characters in word\n",
    "    cw = Counter(word)\n",
    "    # precomputes a set of the different characters\n",
    "    sw = set(cw)\n",
    "    # precomputes the \"length\" of the word vector\n",
    "    lw = sqrt(sum(c*c for c in cw.values()))\n",
    "\n",
    "    # return a tuple\n",
    "    return cw, sw, lw\n",
    "\n",
    "def cosdis(v1, v2):\n",
    "    # which characters are common to the two words?\n",
    "    common = v1[1].intersection(v2[1])\n",
    "    # by definition of cosine distance we have\n",
    "    return sum(v1[0][ch]*v2[0][ch] for ch in common)/v1[2]/v2[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/es1_defs/paolo_defs.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/es1_defs/paolo_defs.ipynb#ch0000011?line=0'>1</a>\u001b[0m list_A \u001b[39m=\u001b[39m list_defs[\u001b[39m0\u001b[39;49m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/es1_defs/paolo_defs.ipynb#ch0000011?line=1'>2</a>\u001b[0m list_B \u001b[39m=\u001b[39m list_defs[\u001b[39m1\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/es1_defs/paolo_defs.ipynb#ch0000011?line=3'>4</a>\u001b[0m threshold \u001b[39m=\u001b[39m \u001b[39m0.80\u001b[39m     \u001b[39m# if needed\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "list_A = list_defs[0]\n",
    "list_B = list_defs[1]\n",
    "\n",
    "threshold = 0.80     # if needed\n",
    "for key in list_A:\n",
    "    for word in list_B:\n",
    "        try:\n",
    "            # print(key)\n",
    "            # print(word)\n",
    "            res = cosdis(word2vec(word), word2vec(key))\n",
    "            # print(res)\n",
    "            print(\"The cosine similarity between : {} and : {} is: {}\".format(word, key, res*100))\n",
    "            # if res > threshold:\n",
    "            #     print(\"Found a word with cosine distance > 80 : {} with original word: {}\".format(word, key))\n",
    "        except IndexError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_A' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\andre\\Desktop\\Università\\Magistrale\\TLN\\PART 3\\tln-2022-third-part-lab\\es1_defs\\defs.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andre/Desktop/Universit%C3%A0/Magistrale/TLN/PART%203/tln-2022-third-part-lab/es1_defs/defs.ipynb#ch0000008?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m Counter\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andre/Desktop/Universit%C3%A0/Magistrale/TLN/PART%203/tln-2022-third-part-lab/es1_defs/defs.ipynb#ch0000008?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmath\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/andre/Desktop/Universit%C3%A0/Magistrale/TLN/PART%203/tln-2022-third-part-lab/es1_defs/defs.ipynb#ch0000008?line=3'>4</a>\u001b[0m counterA \u001b[39m=\u001b[39m Counter(list_A)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andre/Desktop/Universit%C3%A0/Magistrale/TLN/PART%203/tln-2022-third-part-lab/es1_defs/defs.ipynb#ch0000008?line=4'>5</a>\u001b[0m counterB \u001b[39m=\u001b[39m Counter(list_B)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andre/Desktop/Universit%C3%A0/Magistrale/TLN/PART%203/tln-2022-third-part-lab/es1_defs/defs.ipynb#ch0000008?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcounter_cosine_similarity\u001b[39m(c1, c2):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'list_A' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "counterA = Counter(list_A)\n",
    "counterB = Counter(list_B)\n",
    "\n",
    "\n",
    "def counter_cosine_similarity(c1, c2):\n",
    "    terms = set(c1).union(c2)\n",
    "    dotprod = sum(c1.get(k, 0) * c2.get(k, 0) for k in terms)\n",
    "    magA = math.sqrt(sum(c1.get(k, 0)**2 for k in terms))\n",
    "    magB = math.sqrt(sum(c2.get(k, 0)**2 for k in terms))\n",
    "    return dotprod / (magA * magB)\n",
    "\n",
    "print(counterA)\n",
    "print(counterB)\n",
    "print(counter_cosine_similarity(counterA, counterB) * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity with wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for list in lists_of_defs:\\n    no_stop = []\\n    no_stop_defs = {}\\n    for definition in list:\\n        # print(definition)\\n        text_tokens = word_tokenize(definition)\\n        tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\\n        no_stop.append(tokens_without_sw)\\n        # print(tokens_without_sw)\\n    bricks_without_sw = no_stop'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate similarity with wordnet\n",
    "for word in list_defs:\n",
    "    syn1 = wordnet.synsets('hello')[0]\n",
    "    syn2 = wordnet.synsets('selling')[0]\n",
    "    \n",
    "    print (\"hello name :  \", syn1.name())\n",
    "    print (\"selling name :  \", syn2.name())\n",
    "\n",
    "    print(syn1.wup_similarity(syn2))\n",
    "\n",
    "# First version of similarity - not working because the diveder at the end\n",
    "'''sim = 0\n",
    "for item in list_defs:\n",
    "    doc1 = nlp(item)\n",
    "    for item2 in list_defs[1:]:\n",
    "        doc2 = nlp(item2)\n",
    "        sim = sim + doc1.similarity(doc2)\n",
    "    \n",
    "sim = sim / (len(list_defs)*len(list_defs))\n",
    "print(sim)'''\n",
    "\n",
    "'''for list in lists_of_defs:\n",
    "    no_stop = []\n",
    "    no_stop_defs = {}\n",
    "    for definition in list:\n",
    "        # print(definition)\n",
    "        text_tokens = word_tokenize(definition)\n",
    "        tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "        no_stop.append(tokens_without_sw)\n",
    "        # print(tokens_without_sw)\n",
    "    bricks_without_sw = no_stop'''\n",
    "    \n",
    "# emotion_defs:\n",
    "\n",
    "# for definition in emotion_defs:\n",
    "#         text_tokens = word_tokenize(definition)\n",
    "#         tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "#         no_stop.append(tokens_without_sw)\n",
    "#         # print(tokens_without_sw)\n",
    "\n",
    "# list_def_brick = ' '.join([str(item) for item in bricks_without_sw])\n",
    "\n",
    "# print(list_def_brick) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stopwrords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stopwords with nltk.\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "df = pd.DataFrame(pd.read_csv('../def2.csv'))\n",
    "\n",
    "# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n",
    "for i in range (37):\n",
    "    df = df.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "print(df['no_stop_words'])\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\andre/nltk_data'\n    - 'c:\\\\Users\\\\andre\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Users\\\\andre\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\andre\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\andre\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mzip_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     85\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\andre/nltk_data'\n    - 'c:\\\\Users\\\\andre\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Users\\\\andre\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\andre\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\andre\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\andre\\Desktop\\Università\\Magistrale\\TLN\\PART 3\\tln-2022-third-part-lab\\es1_defs\\defs.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/andre/Desktop/Universit%C3%A0/Magistrale/TLN/PART%203/tln-2022-third-part-lab/es1_defs/defs.ipynb#ch0000014?line=0'>1</a>\u001b[0m stop \u001b[39m=\u001b[39m stopwords\u001b[39m.\u001b[39;49mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andre/Desktop/Universit%C3%A0/Magistrale/TLN/PART%203/tln-2022-third-part-lab/es1_defs/defs.ipynb#ch0000014?line=1'>2</a>\u001b[0m nltk\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39mstopwords\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andre/Desktop/Universit%C3%A0/Magistrale/TLN/PART%203/tln-2022-third-part-lab/es1_defs/defs.ipynb#ch0000014?line=3'>4</a>\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: [item \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m x \u001b[39mif\u001b[39;00m item \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stop])\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLazyCorpusLoader object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[0;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfind(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubdir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mzip_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[39m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__reader_cls(root, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     82\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\andre/nltk_data'\n    - 'c:\\\\Users\\\\andre\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Users\\\\andre\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\andre\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\andre\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "df = df.apply(lambda x: [item for item in x if item not in stop])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe from the csv file\n",
    "df = pd.read_csv('../def.csv')\n",
    "\n",
    "df = df.reset_index()  # make sure indexes pair with number of rows\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    print(row['0'], row['1'])\n",
    "\n",
    "# 2 Fare stemming/lemming di ogni frase\n",
    "# 3 calcolare similarità tra tutte le definizioni di una singola categoria\n",
    "'''for line in f:\n",
    "    print(line)'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venvdicaro': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3fc711e4ee2b6550a1e55fb58a23702f691a325dedbbea2e10fa55c056de4b98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
