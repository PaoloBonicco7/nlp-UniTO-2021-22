{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approccio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prendo il termine più frequente nelle definizioni, sarà il genus\n",
    "- Stopwords removing e lemming delle definizioni\n",
    "- Prelevo tutto il sottoalbero di hyponimi del genus\n",
    "- Prendo le definizioni (glossa) dei synset di cui ho trovato i hyponimi\n",
    "- Faccio confronto tra definizioni di wordnet e lista di definizioni\n",
    "- Restituisco il synset che ha definizioni più simile a quella della lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from gensim.test.utils import simple_preprocess\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.wsd import lesk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_sense_disambiguation(list_words, word):\n",
    "    right_synset = lesk(list_words, word)\n",
    "    return right_synset\n",
    "\n",
    "# print(word_sense_disambiguation(['i', 'went', 'to', 'the', 'bank', 'to', 'give', 'money'], 'bank').definition())\n",
    "\n",
    "def remove_stop_words(row):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = [',', '.', ';', '!', '?', \"'\", \"''\", '\"', \"’\", \"’’\", \"`\",\"``\"]\n",
    "    filtered_sentence = [w for w in row if not w.lower() in stop_words and w not in punctuation]\n",
    "    return filtered_sentence\n",
    "\n",
    "def calculate_frequency(row):\n",
    "    freq_dict = {}\n",
    "    row = remove_stop_words(word_tokenize(row))\n",
    "    for word in row[1:]:\n",
    "        if word.lower() not in freq_dict:\n",
    "            freq_dict[word.lower()] = 1\n",
    "        else:\n",
    "            freq_dict[word.lower()] += 1\n",
    "    return freq_dict\n",
    "\n",
    "def get_text_from_file(path):\n",
    "    file = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    with open (path, 'r') as f:\n",
    "        for row in f:\n",
    "            filtered_s = [w for w in word_tokenize(row) if not w.lower() in stop_words]\n",
    "            file.append(simple_preprocess(str(filtered_s), deacc=True))\n",
    "    f.close()\n",
    "    return file\n",
    "\n",
    "def get_hypos(word):\n",
    "    syn = get_synset(word)\n",
    "    hypo_list = list(set([w for s in syn.closure(lambda s:s.hyponyms()) for w in s.lemma_names()]))\n",
    "    return hypo_list\n",
    "\n",
    "\n",
    "def get_synset(word):\n",
    "    if(len(wn.synsets(word)) > 0):\n",
    "        return wn.synsets(word)[0]\n",
    "    return None\n",
    "\n",
    "# Program to measure the similarity between \n",
    "# two sentences using cosine similarity.\n",
    "def cos_similarity(sen1, sen2):\n",
    "    # tokenization\n",
    "    X_list = wn.word_tokenize(sen1) \n",
    "    Y_list = wn.word_tokenize(sen2)\n",
    "    \n",
    "    # sw contains the list of stopwords\n",
    "    sw = stopwords.words('english') \n",
    "    l1 =[];l2 =[]\n",
    "    \n",
    "    # remove stop words from the string\n",
    "    X_set = {w for w in X_list if not w in sw} \n",
    "    Y_set = {w for w in Y_list if not w in sw}\n",
    "    \n",
    "    # form a set containing keywords of both strings \n",
    "    rvector = X_set.union(Y_set) \n",
    "    for w in rvector:\n",
    "        if w in X_set: l1.append(1) # create a vector\n",
    "        else: l1.append(0)\n",
    "        if w in Y_set: l2.append(1)\n",
    "        else: l2.append(0)\n",
    "    c = 0\n",
    "    \n",
    "    # cosine formula \n",
    "    for i in range(len(rvector)):\n",
    "            c+= l1[i]*l2[i]\n",
    "    cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
    "                       \n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penn_to_wn(tag):\n",
    "    \"\"\" Convert between a Penn Treebank tag to a simplified Wordnet tag \"\"\"\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    " \n",
    "    if tag.startswith('V'):\n",
    "        return 'v'\n",
    " \n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    " \n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    " \n",
    "    return None\n",
    " \n",
    "def tagged_to_synset(word, tag):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag is None:\n",
    "        return None\n",
    " \n",
    "    try:\n",
    "        return wn.synsets(word, wn_tag)[0]\n",
    "    except:\n",
    "        return None\n",
    " \n",
    "def sentence_similarity(sentence1, sentence2):\n",
    "    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n",
    "    # Tokenize and tag\n",
    "    sentence1 = pos_tag(word_tokenize(sentence1))\n",
    "    sentence2 = pos_tag(word_tokenize(sentence2))\n",
    " \n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
    "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
    " \n",
    "    # Filter out the Nones\n",
    "    synsets1 = [ss for ss in synsets1 if ss]\n",
    "    synsets2 = [ss for ss in synsets2 if ss]\n",
    " \n",
    "    score, count = 0.0, 0\n",
    " \n",
    "    # For each word in the first sentence\n",
    "    for synset in synsets1:\n",
    "        # Get the similarity value of the most similar word in the other sentence\n",
    "        best_score = max([synset.path_similarity(ss) for ss in synsets2])\n",
    " \n",
    "        # Check that the similarity could have been computed\n",
    "        if best_score is not None:\n",
    "            score += best_score\n",
    "            count += 1\n",
    " \n",
    "    # Average the values\n",
    "    score /= count\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(...)? (653542922.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [8]\u001b[0;36m\u001b[0m\n\u001b[0;31m    print \"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % (focus_sentence, sentence, sentence_similarity(focus_sentence, sentence))\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(...)?\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"Dogs are awesome.\",\n",
    "    \"Some gorgeous creatures are felines.\",\n",
    "    \"Dolphins are swimming mammals.\",\n",
    "    \"Cats are beautiful animals.\",\n",
    "]\n",
    " \n",
    "focus_sentence = \"Cats are beautiful animals.\"\n",
    " \n",
    "for sentence in sentences:\n",
    "    print \"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % (focus_sentence, sentence, sentence_similarity(focus_sentence, sentence))\n",
    "    print \"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % (sentence, focus_sentence, sentence_similarity(sentence, focus_sentence))\n",
    "    print \n",
    " \n",
    "# Similarity(\"Cats are beautiful animals.\", \"Dogs are awesome.\") = 0.511111111111\n",
    "# Similarity(\"Dogs are awesome.\", \"Cats are beautiful animals.\") = 0.666666666667\n",
    " \n",
    "# Similarity(\"Cats are beautiful animals.\", \"Some gorgeous creatures are felines.\") = 0.833333333333\n",
    "# Similarity(\"Some gorgeous creatures are felines.\", \"Cats are beautiful animals.\") = 0.833333333333\n",
    " \n",
    "# Similarity(\"Cats are beautiful animals.\", \"Dolphins are swimming mammals.\") = 0.483333333333\n",
    "# Similarity(\"Dolphins are swimming mammals.\", \"Cats are beautiful animals.\") = 0.4\n",
    " \n",
    "# Similarity(\"Cats are beautiful animals.\", \"Cats are beautiful animals.\") = 1.0\n",
    "# Similarity(\"Cats are beautiful animals.\", \"Cats are beautiful animals.\") = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/venvdicaro/lib/python3.10/site-packages/nltk/corpus/reader/wordnet.py:599: UserWarning: Discarded redundant search for Synset('gusto.n.01') at depth 3\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n",
      "/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/venvdicaro/lib/python3.10/site-packages/nltk/corpus/reader/wordnet.py:599: UserWarning: Discarded redundant search for Synset('antagonism.n.03') at depth 4\n",
      "  for synset in acyclic_breadth_first(self, rel, depth):\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/paolobonicco/nltk_data'\n    - '/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/venvdicaro/nltk_data'\n    - '/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/venvdicaro/share/nltk_data'\n    - '/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/venvdicaro/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/es2_content2form/content2word.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/es2_content2form/content2word.ipynb#ch0000008?line=23'>24</a>\u001b[0m \u001b[39mfor\u001b[39;00m wndef \u001b[39min\u001b[39;00m hypo_list:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/es2_content2form/content2word.ipynb#ch0000008?line=24'>25</a>\u001b[0m     \u001b[39mfor\u001b[39;00m mydef \u001b[39min\u001b[39;00m row[\u001b[39m1\u001b[39m:]:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/es2_content2form/content2word.ipynb#ch0000008?line=25'>26</a>\u001b[0m         score \u001b[39m=\u001b[39m sentence_similarity(mydef, wndef[\u001b[39m1\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/es2_content2form/content2word.ipynb#ch0000008?line=26'>27</a>\u001b[0m         \u001b[39mif\u001b[39;00m score \u001b[39m>\u001b[39m best_score[\u001b[39m0\u001b[39m]:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/es2_content2form/content2word.ipynb#ch0000008?line=27'>28</a>\u001b[0m             best_score \u001b[39m=\u001b[39m (score, wndef[\u001b[39m0\u001b[39m], wndef[\u001b[39m1\u001b[39m])\n",
      "\u001b[1;32m/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/es2_content2form/content2word.ipynb Cell 9\u001b[0m in \u001b[0;36msentence_similarity\u001b[0;34m(sentence1, sentence2)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/es2_content2form/content2word.ipynb#ch0000008?line=27'>28</a>\u001b[0m \u001b[39m\"\"\" compute the sentence similarity using Wordnet \"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/es2_content2form/content2word.ipynb#ch0000008?line=28'>29</a>\u001b[0m \u001b[39m# Tokenize and tag\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/es2_content2form/content2word.ipynb#ch0000008?line=29'>30</a>\u001b[0m sentence1 \u001b[39m=\u001b[39m pos_tag(word_tokenize(sentence1))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/es2_content2form/content2word.ipynb#ch0000008?line=30'>31</a>\u001b[0m sentence2 \u001b[39m=\u001b[39m pos_tag(word_tokenize(sentence2))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/es2_content2form/content2word.ipynb#ch0000008?line=32'>33</a>\u001b[0m \u001b[39m# Get the synsets for the tagged words\u001b[39;00m\n",
      "File \u001b[0;32m~/virtual-envs/tln-2022-third-part-lab/venvdicaro/lib/python3.10/site-packages/nltk/tag/__init__.py:165\u001b[0m, in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpos_tag\u001b[39m(tokens, tagset\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meng\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    141\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[39m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39m    tag the given list of tokens.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m     tagger \u001b[39m=\u001b[39m _get_tagger(lang)\n\u001b[1;32m    166\u001b[0m     \u001b[39mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[0;32m~/virtual-envs/tln-2022-third-part-lab/venvdicaro/lib/python3.10/site-packages/nltk/tag/__init__.py:107\u001b[0m, in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    105\u001b[0m     tagger\u001b[39m.\u001b[39mload(ap_russian_model_loc)\n\u001b[1;32m    106\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     tagger \u001b[39m=\u001b[39m PerceptronTagger()\n\u001b[1;32m    108\u001b[0m \u001b[39mreturn\u001b[39;00m tagger\n",
      "File \u001b[0;32m~/virtual-envs/tln-2022-third-part-lab/venvdicaro/lib/python3.10/site-packages/nltk/tag/perceptron.py:167\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[0;34m(self, load)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m load:\n\u001b[1;32m    166\u001b[0m     AP_MODEL_LOC \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile:\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\n\u001b[0;32m--> 167\u001b[0m         find(\u001b[39m\"\u001b[39;49m\u001b[39mtaggers/averaged_perceptron_tagger/\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m PICKLE)\n\u001b[1;32m    168\u001b[0m     )\n\u001b[1;32m    169\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload(AP_MODEL_LOC)\n",
      "File \u001b[0;32m~/virtual-envs/tln-2022-third-part-lab/venvdicaro/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/paolobonicco/nltk_data'\n    - '/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/venvdicaro/nltk_data'\n    - '/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/venvdicaro/share/nltk_data'\n    - '/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/venvdicaro/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "with open ('../res/def.csv', 'r') as f:\n",
    "    for row in f:\n",
    "        dict = calculate_frequency(row)\n",
    "        c = Counter(dict)\n",
    "        \n",
    "        most_common = c.most_common(5)\n",
    "        \n",
    "        # The genus is the most used word in the definitions\n",
    "        genus = c.most_common(1)\n",
    "        \n",
    "        # Compongo lista parole più usate nelle definizioni da confrontare con definizioni wordnet\n",
    "        my_key_word = []\n",
    "        for el in most_common:\n",
    "            my_key_word.append(el[0])\n",
    "        \n",
    "        # List of hyponyms of the genus and their wordnet definition\n",
    "        hypo_list = get_hypos(genus[0][0])\n",
    "        hypo_def = []\n",
    "        for hypo in hypo_list:\n",
    "            hypo_def.append((hypo, get_synset(hypo).definition()))\n",
    "        \n",
    "        best_score = (0, \"\", \"\") # (score, word, definition)\n",
    "            \n",
    "        for wndef in hypo_list:\n",
    "            for mydef in row[1:]:\n",
    "                score = sentence_similarity(mydef, wndef[1])\n",
    "                if score > best_score[0]:\n",
    "                    best_score = (score, wndef[0], wndef[1])\n",
    "                    \n",
    "        print(f'best_score is {best_score} and the real word is {row[0]}')\n",
    "             \n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venvdicaro': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3fc711e4ee2b6550a1e55fb58a23702f691a325dedbbea2e10fa55c056de4b98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
