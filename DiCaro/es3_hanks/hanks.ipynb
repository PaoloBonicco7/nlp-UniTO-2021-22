{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esercizio 3 - Hanks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consegna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scegliere un verbo transitivo --> **KILL**\n",
    "  \n",
    "- Trovare un corpus con > 1000 frasi in cui comprare un verbo scelto (usare un verbo comune) --> link to resource: https://sentence.yourdictionary.com/kill\n",
    "- Effettuare parsing e disambiguazione\n",
    "- Usare i supersensi di wordnet sugli argomenti (subj e obj nel caso di 2 argomenti) del verbo scelto\n",
    "- Calcolo risultati, frequenza e stampare cluster semantici ottenuti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approccio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partendo da un dataset di frasi con il verbo *kill* abbiamo estratto, tramite il *Dependency Matcher* di *spacy*, \n",
    "l'oggetto e il soggetto del verbo.\n",
    "\n",
    "Dopo di che abbiamo ricavato i synset di entrambi.\n",
    "\n",
    "Visto che il nostro dataset presentava molte frasi in cui soggeto o oggetto erano parole come *\"you\", \"me\", \"someone\",...*\n",
    "abbiamo automaticamente associato come synset *person* a tutti i termini simili a quelli indicati precedentemente.\n",
    "(La lista completa è presente nella lista \"person\")\n",
    "\n",
    "Infine abbiamo stampato le statistiche del nostro dataset, indicando la percentuale di volte in cui il verbo kill comprare\n",
    "con soggetto/verbo appartenenti alla stessa categoria, o meglio che si riconducono allo stesso synset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Come distinguo i synset?**\n",
    "--> Funzione lesk (https://www.nltk.org/howto/wsd.html#word-sense-disambiguation)\n",
    "\n",
    "**Come trovo i supersensi?**\n",
    "--> Funzione lexname (https://wordnet.princeton.edu/documentation/lexnames5wn)\n",
    "I Synsets in wordnet sono organizzati sotto 45 categorie, abbiamo deciso di utilizzare questa funzione per estrarre le categorie semantiche a cui associare\n",
    "i soggetti e gli oggetti estratti dalle frasi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esecuzione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from spacy.matcher import DependencyMatcher\n",
    "from nltk.wsd import lesk\n",
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* person: Usata per trovare il supersenso dei pronomi\n",
    "* patter: Usato per trovare il soggetto e l'oggetto del verbo kill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = [\"i\", \"you\", \"he\", \"she\", \"we\", \"they\", \"me\", \"him\", \"her\", \"his\", \"them\", \"someone\", \"us\", \"people\", \"anyone\"] \n",
    "\n",
    "pattern1 = [\n",
    "    {\"RIGHT_ID\": \"attr\",\n",
    "    \"RIGHT_ATTRS\": {\"LEMMA\": {\"IN\": [\"kill\"]}}\n",
    "    },\n",
    "    {\"LEFT_ID\": \"attr\",\n",
    "    \"REL_OP\": \">\",\n",
    "    \"RIGHT_ID\": \"subj\",\n",
    "    \"RIGHT_ATTRS\": {\"DEP\": {\"IN\": [\"nsubj\"]}}\n",
    "    },\n",
    "    {\"LEFT_ID\": \"attr\",\n",
    "    \"REL_OP\": \">\",\n",
    "    \"RIGHT_ID\": \"dobj\",\n",
    "    \"RIGHT_ATTRS\": {\"DEP\": {\"IN\": [\"dobj\"]}}\n",
    "    }\n",
    "]\n",
    "\n",
    "pattern2 = [\n",
    "    {\"RIGHT_ID\": \"verb\",\n",
    "    \"RIGHT_ATTRS\": {\"LEMMA\": {\"IN\": [\"want\", \"wish\"]}}\n",
    "    },\n",
    "    {\"LEFT_ID\": \"verb\",\n",
    "    \"REL_OP\": \">\",\n",
    "    \"RIGHT_ID\": \"subj\",\n",
    "    \"RIGHT_ATTRS\": {\"DEP\": {\"IN\": [\"nsubj\"]}}\n",
    "    },\n",
    "    {\"LEFT_ID\": \"verb\",\n",
    "    \"REL_OP\": \">\",\n",
    "    \"RIGHT_ID\": \"xcomp\",\n",
    "    \"RIGHT_ATTRS\": {\"DEP\": {\"IN\": [\"xcomp\"]}}\n",
    "    },\n",
    "    {\"LEFT_ID\": \"xcomp\",\n",
    "    \"REL_OP\": \">\",\n",
    "    \"RIGHT_ID\": \"dobj\",\n",
    "    \"RIGHT_ATTRS\": {\"DEP\": {\"IN\": [\"dobj\"]}}\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carico spacy e aggiungo il pattern al Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "matcher = DependencyMatcher(nlp.vocab)\n",
    "matcher.add(\"pattern1\", [pattern1])\n",
    "matcher.add(\"pattern2\", [pattern2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metodi per trovare il soggetto e l'oggetto del verbo e per fare word sense disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match(text):\n",
    "    # Find the pattern in the document\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    for match in matches:\n",
    "        match_words = sorted(match[1])\n",
    "        phrase = doc[match_words[0]:match_words[len(match_words)-1]+1]\n",
    "        subj = phrase[0].text\n",
    "        dobj = phrase[len(phrase)-1].text\n",
    "        \n",
    "        return subj,dobj,phrase[0].tag_,phrase[len(phrase)-1].tag_\n",
    "    return \"\",\"\",\"\",\"\"\n",
    "\n",
    "def word_sense_disambiguation(list_words, word):\n",
    "    right_synset = lesk(list_words, word)\n",
    "    return right_synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "    res = text.split('.')\n",
    "    return res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('artifact', 'communication'): 1.16 %\n",
      "('person', 'person'): 70.52 %\n",
      "('person', 'group'): 1.73 %\n",
      "('person', 'communication'): 2.89 %\n",
      "('person', 'animal'): 3.47 %\n",
      "('act', 'person'): 1.16 %\n",
      "('person', 'all'): 5.2 %\n",
      "('person', 'artifact'): 2.31 %\n",
      "('cognition', 'person'): 1.73 %\n",
      "('person', 'cognition'): 1.16 %\n",
      "('person', 'act'): 1.73 %\n",
      "('group', 'person'): 3.47 %\n",
      "('contact', 'person'): 1.16 %\n",
      "('person', 'emotion'): 1.16 %\n",
      "('social', 'person'): 1.16 %\n",
      "Somma delle percentuali: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "subj_ss = \"\"\n",
    "dobj_ss = \"\"\n",
    "struct = {}\n",
    "tot = 0\n",
    "c = 0\n",
    "with open ('../data/sentence_kill.txt', 'r', encoding=\"utf8\") as f:\n",
    "    for row in f:\n",
    "        subj_synset, dobj_synset, subj_ss, dobj_ss = None, None, \"\", \"\"\n",
    "        subj, dobj, stag, dtag = get_match(row)\n",
    "            \n",
    "        if subj != \"\" and dobj != \"\":\n",
    "            # Cerco il synset del soggetto e dell'oggetto e associo automaticamente il synset \"person\" se trovo\n",
    "            # un nome proprio o una sringa presente in person\n",
    "            \n",
    "            # Soggetto\n",
    "            if stag == \"NNP\" or subj.lower() in person:\n",
    "                subj_ss = \"person\"\n",
    "            else:\n",
    "                subj_synset = word_sense_disambiguation(re.findall(r'\\w+', row), subj)\n",
    "                \n",
    "            # Oggetto\n",
    "            if dtag == \"NNP\" or dobj.lower() in person:\n",
    "                dobj_ss = \"person\"\n",
    "            else:\n",
    "                dobj_synset = word_sense_disambiguation(re.findall(r'\\w+', row), dobj)        \n",
    "            \n",
    "            # Soggetto - Se subj_synset e' None, significa che abbiamo associato il synset person\n",
    "            if not subj_synset is None:\n",
    "                subj_ss = cleaner(subj_synset.lexname())\n",
    "            elif subj_ss != \"person\":\n",
    "                subj_ss = \"unknown\"\n",
    "            \n",
    "            # Oggetto\n",
    "            if not dobj_synset is None:\n",
    "                dobj_ss = cleaner(dobj_synset.lexname())  \n",
    "            elif dobj_ss != \"person\":\n",
    "                dobj_ss = \"unknown\"\n",
    "\n",
    "            if (subj_ss, dobj_ss) in struct:\n",
    "                struct[(subj_ss, dobj_ss)] += 1\n",
    "            else:\n",
    "                struct[(subj_ss, dobj_ss)] = 1\n",
    "        else:\n",
    "            c+=1\n",
    "            #print(f\"phrase {c}: {row}\")\n",
    "\n",
    "# Used to check the number of sentences with no match\n",
    "for (k,v) in zip(struct.keys(),struct.values()):\n",
    "    if (k[0] != \"unknown\" and k[1] != \"unknown\") and v > 1:\n",
    "        tot += v\n",
    "\n",
    "somma_tot = 0\n",
    "for k in struct.keys():\n",
    "    if (k[0] != \"unknown\" and k[1] != \"unknown\") and struct[k] > 1:\n",
    "        print(f\"{k}: {round(((struct[k]/tot)*100), 2)} %\")\n",
    "        somma_tot += ((struct[k]/tot)*100)\n",
    "print(f\"Somma delle percentuali: {round(somma_tot, 3)} %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisi dei risultati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le percentuali ottenute indicano la frequenza con cui kill compare con un soggetto/oggetto appartenente ad una determinata categoria.\n",
    "\n",
    "Possiamo notare che il **70%** delle volte compare con **person** sia come oggetto che come soggetto, il che è abbastanza plausibile se \n",
    "andiamo ad analizzare il dataset che abbiamo utilizzato e come abbiamo determinato il tipo *person*.\n",
    "\n",
    "L'approccio è abbastanza scalabile siccome il *dependency matcher* è abbastanza veloce, ma potrebbero servire altri pattern in base al dataset \n",
    "utilizzato.\n",
    "\n",
    "Questo esperimento ha evidenziato come sia difficile disambiguare un verbo per un calcolatore, vedendo la quantità di tipi diversi che abbiamo \n",
    "individuato, ma anche come l'approccio di Patrick Hanks sia molto semplice da eseguire e produca risultati molto precisi.\n",
    "\n",
    "**Cosa si potrebbe migliorare?**\n",
    "\n",
    "- *Word Sense Disambiguation*: Per fare WSD e trovare il synset corretto abbiamo utilizzato la funzione lesk di wordnet che come sappiamo\n",
    "  non funziona perfettamente e potrebbe restituire un synset non corretto\n",
    "\n",
    "- *Individuazione supersensi*: Abbiamo utilizzato la funzione lexname() per individuare i supersensi degli argomenti del verbo, è possibile che utilizzando\n",
    "  altri approcci si ottengano risultati diversi.\n",
    "\n",
    "- *Dataset*: Il dataset che abbiamo utilizzato contiene poche frasi e quindi non è un campione significativo del verbo *kill*, anche se \n",
    "  nonostante ciò siamo riusciti a catturare una discreta quantità di sensi diversi.\n",
    "\n",
    "- *Significati del verbo*: Sulla base dei risultati ottenuti si potrebbe associare un senso al verbo sulla base della categoria dell'oggetto e del soggetto,\n",
    "  ad esempio *'person' kill 'person'* potrebbe rappresentare l'azione di uccidere inteso come togliere la vita ad una persona, mentre\n",
    "  *'person' kill 'artifactal'* potrebbe rappresentare l'azione terminare un processo, come ad esempo *\"kill a rumor\"* (spegnere la luce).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venvdicaro': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3fc711e4ee2b6550a1e55fb58a23702f691a325dedbbea2e10fa55c056de4b98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
